{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b9516e6b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T09:18:39.351556Z",
     "iopub.status.busy": "2025-12-29T09:18:39.350970Z",
     "iopub.status.idle": "2025-12-29T09:18:39.357492Z",
     "shell.execute_reply": "2025-12-29T09:18:39.356772Z",
     "shell.execute_reply.started": "2025-12-29T09:18:39.351519Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Agri-Foundational Hurdle Network (AFHN)\n",
    "==================================================================\n",
    "Production-ready PyTorch implementation for pasture biomass prediction\n",
    "using Zero-Inflated LogNormal loss, DINOv2 backbone, and MIL aggregation.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import math\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "from transformers import AutoModel, AutoConfig\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f8146c35",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T09:18:39.359201Z",
     "iopub.status.busy": "2025-12-29T09:18:39.358938Z",
     "iopub.status.idle": "2025-12-29T09:18:39.376086Z",
     "shell.execute_reply": "2025-12-29T09:18:39.375495Z",
     "shell.execute_reply.started": "2025-12-29T09:18:39.359181Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CONSTANTS & CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "# DINOv2 native resolution\n",
    "IMG_SIZE = 518\n",
    "PATCH_STRIDE = 400  # ~23% overlap for smoother features\n",
    "ORIG_H, ORIG_W = 1000, 2000\n",
    "DINO_PATH = '/kaggle/input/dinov2/pytorch/large/1'\n",
    "\n",
    "# Hyperparameters\n",
    "BATCH_SIZE = 4\n",
    "LEARNING_RATE = 1e-4\n",
    "WEIGHT_DECAY = 1e-5\n",
    "NUM_EPOCHS = 2\n",
    "PATIENCE = 10\n",
    "\n",
    "# Target names and weights (as per competition)\n",
    "TARGET_NAMES = ['Dry_Green_g', 'Dry_Dead_g', 'Dry_Clover_g', 'GDM_g', 'Dry_Total_g']\n",
    "TARGET_WEIGHTS = {\n",
    "    'Dry_Green_g': 0.1,\n",
    "    'Dry_Dead_g': 0.1,\n",
    "    'Dry_Clover_g': 0.1,\n",
    "    'GDM_g': 0.2,\n",
    "    'Dry_Total_g': 0.5\n",
    "}\n",
    "\n",
    "# State encoding (one-hot)\n",
    "STATES = ['NSW', 'Tas', 'Vic', 'WA']\n",
    "STATE_TO_IDX = {s: i for i, s in enumerate(STATES)}\n",
    "\n",
    "# Dataset paths\n",
    "TRAIN_CSV = '/kaggle/input/csiro-biomass/train.csv'\n",
    "TRAIN_IMG_DIR = '/kaggle/input/csiro-biomass/'\n",
    "\n",
    "TEST_CSV = '/kaggle/input/csiro-biomass/test.csv'\n",
    "TEST_IMG_DIR = 'test_images/'\n",
    "MODEL_PATH = 'afhn_fold0_best.pth' \n",
    "\n",
    "# Device configuration\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29b4ac4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T09:18:39.377116Z",
     "iopub.status.busy": "2025-12-29T09:18:39.376862Z",
     "iopub.status.idle": "2025-12-29T09:18:39.387774Z",
     "shell.execute_reply": "2025-12-29T09:18:39.387207Z",
     "shell.execute_reply.started": "2025-12-29T09:18:39.377096Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# LOSS FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "class ZeroInflatedLogNormalLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Zero-Inflated LogNormal (ZILN) Loss for semi-continuous targets.\n",
    "    \n",
    "    Combines:\n",
    "    1. Binary classification (zero vs non-zero)\n",
    "    2. LogNormal regression for positive values\n",
    "    \n",
    "    Model outputs 3 values per target:\n",
    "    - logit_prob: Logit for P(y > 0)\n",
    "    - mu: Mean of log-normal distribution\n",
    "    - sigma_raw: Raw scale parameter (transformed via softplus)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "    \n",
    "    def forward(self, preds: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            preds: (batch, 3) - [logit_prob, mu, sigma_raw]\n",
    "            target: (batch,) - Ground truth values (>= 0)\n",
    "        \n",
    "        Returns:\n",
    "            Scalar loss value\n",
    "        \"\"\"\n",
    "        logit_prob = preds[:, 0]\n",
    "        mu = preds[:, 1]\n",
    "        sigma_raw = preds[:, 2]\n",
    "        \n",
    "        # Ensure sigma > 0\n",
    "        sigma = F.softplus(sigma_raw) + self.eps\n",
    "\n",
    "        # Binary indicator: 1 if y > 0, else 0\n",
    "        is_positive = (target > 0).float()\n",
    "        \n",
    "        # Classification loss (zero vs non-zero)\n",
    "        class_loss = F.binary_cross_entropy_with_logits(\n",
    "            logit_prob, is_positive, reduction='none'\n",
    "        )\n",
    "        \n",
    "        # Regression loss (LogNormal NLL for positives)\n",
    "        safe_target = torch.clamp(target, min=self.eps)\n",
    "        log_target = torch.log(safe_target)\n",
    "        \n",
    "        reg_loss = (\n",
    "            log_target +\n",
    "            torch.log(sigma) +\n",
    "            0.5 * math.log(2 * math.pi) +\n",
    "            (log_target - mu).pow(2) / (2 * sigma.pow(2))\n",
    "        )\n",
    "        \n",
    "        # Combined loss: classification always, regression only for positives\n",
    "        total_loss = class_loss + (is_positive * reg_loss)\n",
    "        \n",
    "        return total_loss.mean()\n",
    "\n",
    "class TweedieLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Tweedie loss for compound Poisson-Gamma distribution.\n",
    "    Suitable for strictly positive, heavy-tailed targets like Total Biomass.\n",
    "    \"\"\"\n",
    "    def __init__(self, p: float = 1.5, epsilon: float = 1e-8):\n",
    "        super().__init__()\n",
    "        assert 1 < p < 2, \"Tweedie power p must be in (1, 2)\"\n",
    "        self.p = p # Power parameter: 1 < p < 2 for Compound Poisson-Gamma\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def forward(self, pred: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            pred: Predicted mean (batch,)\n",
    "            target: Ground truth (batch,)\n",
    "        \"\"\"\n",
    "        # Ensure positivity\n",
    "        pred = torch.clamp(pred, min=self.epsilon) \n",
    "        target = torch.clamp(target, min=self.epsilon) \n",
    "        \n",
    "        # Tweedie deviance\n",
    "        term1 = -target * torch.pow(pred, 1 - self.p) / (1 - self.p)\n",
    "        term2 = torch.pow(pred, 2 - self.p) / (2 - self.p)\n",
    "        \n",
    "        loss = term1 + term2\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "22a189e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T09:18:39.389525Z",
     "iopub.status.busy": "2025-12-29T09:18:39.389228Z",
     "iopub.status.idle": "2025-12-29T09:18:39.400427Z",
     "shell.execute_reply": "2025-12-29T09:18:39.399814Z",
     "shell.execute_reply.started": "2025-12-29T09:18:39.389507Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# MULTIPLE INSTANCE LEARNING (MIL) MODULE\n",
    "# ============================================================================\n",
    "\n",
    "class GatedAttentionMIL(nn.Module):\n",
    "    \"\"\"\n",
    "    Gated Attention mechanism for MIL aggregation.\n",
    "    \n",
    "    Based on: Ilse et al. (2018) \"Attention-based Deep Multiple Instance Learning\"\n",
    "    Equation: a_k = softmax(w^T * (tanh(V*h_k) ⊙ sigmoid(U*h_k)))\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim: int = 1024, hidden_dim: int = 256):\n",
    "        super().__init__()\n",
    "        self.attention_V = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.attention_U = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.attention_weights = nn.Linear(hidden_dim, 1)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (batch, n_patches, embed_dim)\n",
    "        \n",
    "        Returns:\n",
    "            aggregated: (batch, embed_dim) - Weighted sum of patches\n",
    "            attention: (batch, n_patches, 1) - Attention weights per patch\n",
    "        \"\"\"\n",
    "\n",
    "        # Gated attention computation\n",
    "        A_V = self.attention_V(x) # (batch, n_patches, hidden_dim)\n",
    "        A_U = self.attention_U(x) # (batch, n_patches, hidden_dim)\n",
    "\n",
    "        # Element-wise gating\n",
    "        A = self.attention_weights(A_V * A_U) # (batch, n_patches, 1)\n",
    "\n",
    "        # Softmax over patches\n",
    "        A = torch.softmax(A, dim=1) # (batch, n_patches, 1)\n",
    "\n",
    "        # Weighted aggregation\n",
    "        M = torch.bmm(A.transpose(1, 2), x) # (batch, 1, embed_dim)\n",
    "\n",
    "        return M.squeeze(1), A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "90cdb5eb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T09:18:39.507438Z",
     "iopub.status.busy": "2025-12-29T09:18:39.507231Z",
     "iopub.status.idle": "2025-12-29T09:18:39.512581Z",
     "shell.execute_reply": "2025-12-29T09:18:39.511722Z",
     "shell.execute_reply.started": "2025-12-29T09:18:39.507420Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# FEATURE-WISE LINEAR MODULATION (FiLM) LAYER\n",
    "# ============================================================================\n",
    "\n",
    "class FiLM(nn.Module):\n",
    "    \"\"\"\n",
    "    Feature-wise Linear Modulation for metadata injection.\n",
    "    \n",
    "    Modulates visual features with metadata (state, height, NDVI):\n",
    "    z_mod = γ(m) ⊙ z + β(m)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, meta_dim: int, feat_dim: int):\n",
    "        super().__init__()\n",
    "        self.scale = nn.Linear(meta_dim, feat_dim)\n",
    "        self.shift = nn.Linear(meta_dim, feat_dim)\n",
    "    \n",
    "    def forward(self, features: torch.Tensor, metadata: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            features: (batch, feat_dim)\n",
    "            metadata: (batch, meta_dim)\n",
    "        \n",
    "        Returns:\n",
    "            modulated: (batch, feat_dim)\n",
    "        \"\"\"\n",
    "        gamma = self.scale(metadata) # (batch, feat_dim)\n",
    "        beta = self.shift(metadata)  # (batch, feat_dim)\n",
    "\n",
    "        return features * gamma + beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "38168a6a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T09:18:39.514001Z",
     "iopub.status.busy": "2025-12-29T09:18:39.513769Z",
     "iopub.status.idle": "2025-12-29T09:18:39.530220Z",
     "shell.execute_reply": "2025-12-29T09:18:39.529552Z",
     "shell.execute_reply.started": "2025-12-29T09:18:39.513971Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# AGRI-FOUNDATIONAL HURDLE NETWORK (AFHN) MODEL\n",
    "# ============================================================================\n",
    "\n",
    "class AFHN(nn.Module):\n",
    "    \"\"\"\n",
    "    Agri-Foundational Hurdle Network for biomass prediction.\n",
    "    \n",
    "    Architecture:\n",
    "    1. DINOv2-Large (frozen) backbone for patch features\n",
    "    2. Gated Attention MIL for patch aggregation\n",
    "    3. FiLM layers for metadata modulation\n",
    "    4. Hierarchical heads with physical constraints\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_components: int = 5,\n",
    "        meta_dim: int = 10,\n",
    "        use_ziln: bool = True \n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_components = num_components\n",
    "        self.use_ziln = use_ziln\n",
    "\n",
    "        # 1. Visual backbone: DINOv2-Large (frozen)\n",
    "        print(\"Loading DINOv2-Large backbone from local storage...\")\n",
    "        # Load Config and Model offline\n",
    "        config = AutoConfig.from_pretrained(DINO_PATH)\n",
    "        self.backbone = AutoModel.from_pretrained(DINO_PATH, config=config)\n",
    "\n",
    "        # Freeze all backbone parameters\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        self.vis_dim = 1024  # DINOv2-Large embedding dimension\n",
    "\n",
    "        # 2. MIL Aggregator\n",
    "        self.mil = GatedAttentionMIL(input_dim=self.vis_dim, hidden_dim=256)\n",
    "\n",
    "        # 3. Metadata Encoder + FiLM\n",
    "        self.meta_encoder = nn.Sequential(\n",
    "            nn.Linear(meta_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 64)\n",
    "        )\n",
    "        self.film = FiLM(meta_dim=64, feat_dim=self.vis_dim)\n",
    "\n",
    "        # 4. Prediction Heads\n",
    "\n",
    "        # A. Total Biomass Head (ZILN or Tweedie)\n",
    "        if self.use_ziln:\n",
    "            self.head_total = nn.Sequential(\n",
    "                nn.Linear(self.vis_dim, 256),\n",
    "                nn.GELU(),\n",
    "                nn.Dropout(0.2),\n",
    "                nn.Linear(256, 3)  # [logit_prob, mu, sigma_raw]\n",
    "            )\n",
    "        else:\n",
    "            self.head_total = nn.Sequential(\n",
    "                nn.Linear(self.vis_dim, 256),\n",
    "                nn.GELU(),\n",
    "                nn.Dropout(0.2),\n",
    "                nn.Linear(256, 1),\n",
    "                nn.Softplus()  # Ensure positive prediction\n",
    "            )\n",
    "        \n",
    "        # B. Component Ratios Head (Softmax for sum-to-one constraint)\n",
    "        self.head_ratios = nn.Sequential(\n",
    "            nn.Linear(self.vis_dim, 256),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, num_components)\n",
    "        )\n",
    "\n",
    "        # C. Component Gates (Hurdle mechanism)\n",
    "        self.head_gate = nn.Sequential(\n",
    "            nn.Linear(self.vis_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, num_components)\n",
    "        )\n",
    "    \n",
    "    def forward_features(\n",
    "        self,\n",
    "        x_patches: torch.Tensor,\n",
    "        metadata: torch.Tensor\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Extract and aggregate patch features.\n",
    "        \n",
    "        Args:\n",
    "            x_patches: (batch, n_patches, 3, H, W)\n",
    "            metadata: (batch, meta_dim)\n",
    "        \n",
    "        Returns:\n",
    "            modulated_feat: (batch, vis_dim)\n",
    "            attn_weights: (batch, n_patches, 1)\n",
    "        \"\"\"\n",
    "        batch_size, n_patches, c, h, w = x_patches.shape\n",
    "\n",
    "        # Flaten patches for batch processing\n",
    "        x_flat = x_patches.view(batch_size * n_patches, c, h, w) # (B*n_patches, 3, H, W)\n",
    "\n",
    "        # Extract patch features (frozen backbone, no grad)\n",
    "        with torch.no_grad():\n",
    "            # Transformers expects 'pixel_values' argument\n",
    "            outputs = self.backbone(pixel_values=x_flat)\n",
    "            \n",
    "            # Get last_hidden_state: (B*n_patches, sequence_length, hidden_size)\n",
    "            last_hidden_state = outputs.last_hidden_state\n",
    "            \n",
    "            # The CLS token is usually at index 0\n",
    "            feat_flat = last_hidden_state[:, 0, :]  # (batch*n_patches, embed_dim)\n",
    "\n",
    "        # Reshape to (batch, n_patches, embed_dim)\n",
    "        feat_seq = feat_flat.view(batch_size, n_patches, -1)\n",
    "\n",
    "        # MIL aggregation\n",
    "        global_feat, attn_weights = self.mil(feat_seq) # (batch, vis_dim), (batch, n_patches, 1)\n",
    "\n",
    "        # Meta Modulation via FiLM\n",
    "        meta_emb = self.meta_encoder(metadata) # (batch, meta_feat_dim)\n",
    "        modulated_feat = self.film(global_feat, meta_emb) # (batch, vis_dim)\n",
    "\n",
    "        return modulated_feat, attn_weights\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x_patches: torch.Tensor,\n",
    "        metadata: torch.Tensor\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Forward pass returning all predictions.\n",
    "        \n",
    "        Returns:\n",
    "            Dict with keys: 'total', 'ratios', 'gates'\n",
    "        \"\"\"\n",
    "        feat, _ = self.forward_features(x_patches, metadata)\n",
    "        \n",
    "        total = self.head_total(feat)\n",
    "        raw_ratios = self.head_ratios(feat)\n",
    "        ratios = F.softmax(raw_ratios, dim=1)\n",
    "        gate_logits = self.head_gate(feat)\n",
    "        \n",
    "        return {\n",
    "            'total': total,\n",
    "            'ratios': ratios,\n",
    "            'gates': gate_logits\n",
    "        }\n",
    "    \n",
    "    def predict_components(\n",
    "        self,\n",
    "        x_patches: torch.Tensor,\n",
    "        metadata: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Inference: predict all 5 component biomass values.\n",
    "        \n",
    "        Returns:\n",
    "            components: (batch, 5) - Predicted biomass for each component\n",
    "        \"\"\"\n",
    "        outputs = self.forward(x_patches, metadata)\n",
    "\n",
    "        # Decode total biomass\n",
    "        if self.use_ziln:\n",
    "            total_ziln = outputs['total']\n",
    "            prob_nonzero = torch.sigmoid(total_ziln[:, 0])\n",
    "            mu = total_ziln[:, 1]\n",
    "            sigma = F.softplus(total_ziln[:, 2]) + 1e-6\n",
    "            # Expected valud of ZILN\n",
    "            expected_total = prob_nonzero * torch.exp(mu +0.5 * sigma.pow(2))\n",
    "        else:\n",
    "            expected_total = outputs['total'].squeeze(-1)\n",
    "        \n",
    "        # Decode gates (Bernoulli probability)\n",
    "        gates = torch.sigmoid(outputs['gates'])\n",
    "\n",
    "        # Apply gates to ratios and renormalize\n",
    "        gated_ratios = outputs['ratios'] * gates\n",
    "        sum_gated = gated_ratios.sum(dim=1, keepdim=True) + 1e-6\n",
    "        final_ratios = gated_ratios / sum_gated\n",
    "\n",
    "        # Component predictions (enforces sum constraint)\n",
    "        components = expected_total.unsqueeze(1) * final_ratios\n",
    "\n",
    "        return components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6636b65e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T09:18:39.531454Z",
     "iopub.status.busy": "2025-12-29T09:18:39.531130Z",
     "iopub.status.idle": "2025-12-29T09:18:39.546010Z",
     "shell.execute_reply": "2025-12-29T09:18:39.545384Z",
     "shell.execute_reply.started": "2025-12-29T09:18:39.531434Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DATASET CLASS\n",
    "# ============================================================================\n",
    "\n",
    "class PatchBiomassDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for CSIRO Biomass with on-the-fly patch extraction.\n",
    "    \n",
    "    Each sample contains:\n",
    "    - Image patches (multiple instances)\n",
    "    - Metadata (height, NDVI, state, species, date)\n",
    "    - Target biomass values (5 components)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        image_dir: str,\n",
    "        transform: Optional[A.Compose] = None,\n",
    "        patch_size: int = IMG_SIZE,\n",
    "        stride: int = PATCH_STRIDE,\n",
    "        is_test: bool = False\n",
    "    ):\n",
    "        # For test set, group by image_path (5 rows per image)\n",
    "        if is_test:\n",
    "            self.df = df.groupby('image_path').first().reset_index()\n",
    "        else:\n",
    "            # For train, group by sample_id (each image appears once)\n",
    "            self.df = df.groupby('sample_id').first().reset_index()\n",
    "        \n",
    "        self.image_dir = Path(image_dir)\n",
    "        self.transform = transform\n",
    "        self.patch_size = patch_size\n",
    "        self.stride = stride\n",
    "        self.is_test = is_test\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.df)\n",
    "\n",
    "    def extract_patches(self, image: np.ndarray) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Extract overlapping patches from high-res image.\n",
    "        \n",
    "        Args:\n",
    "            image: (H, W, 3) numpy array\n",
    "        \n",
    "        Returns:\n",
    "            patches: (n_patches, 3, patch_size, patch_size)\n",
    "        \"\"\"\n",
    "        h, w = image.shape[:2]\n",
    "        patches = []\n",
    "\n",
    "        for y in range(0, h - self.patch_size + 1, self.stride):\n",
    "            for x in range(0, w - self.patch_size + 1, self.stride):\n",
    "                patch = image[y:y+self.patch_size, x:x+self.patch_size]\n",
    "                \n",
    "                if self.transform:\n",
    "                    augmented = self.transform(image=patch)\n",
    "                    patch_tensor = augmented['image']\n",
    "                else:\n",
    "                    patch_tensor = torch.from_numpy(patch).permute(2, 0, 1).float() / 255.0\n",
    "                \n",
    "                patches.append(patch_tensor)\n",
    "        \n",
    "        return torch.stack(patches)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict:\n",
    "        row = self.df.iloc[idx]\n",
    "\n",
    "        # Load image\n",
    "        img_path = self.image_dir / row['image_path']\n",
    "        image = np.array(Image.open(img_path).convert('RGB'))\n",
    "\n",
    "        # Extract patches\n",
    "        patches = self.extract_patches(image)\n",
    "\n",
    "        # Encode metadata\n",
    "        # State one-hot\n",
    "        state_onehot = np.zeros(len(STATES))\n",
    "        if row['State'] in STATE_TO_IDX:\n",
    "            state_onehot[STATE_TO_IDX[row['State']]] = 1.0\n",
    "\n",
    "        # Date encoding (cyclical)\n",
    "        date = pd.to_datetime(row['Sampling_Date'])\n",
    "        month = date.month\n",
    "        month_sin = np.sin(2 * np.pi * month / 12)\n",
    "        month_cos = np.cos(2 * np.pi * month / 12)\n",
    "\n",
    "        # Combine metadata: [height, ndvi, state_onehot(4), month_sin, month_cos]\n",
    "        metadata = np.concatenate([\n",
    "            [row['Height_Ave_cm'], row['Pre_GSHH_NDVI']],\n",
    "            state_onehot,\n",
    "            [month_sin, month_cos]\n",
    "        ]).astype(np.float32)\n",
    "\n",
    "        metadata = torch.from_numpy(metadata)\n",
    "\n",
    "        # Targets (if not test)\n",
    "        if not self.is_test:\n",
    "            targets = torch.tensor([\n",
    "                row['Dry_Green_g'],\n",
    "                row['Dry_Dead_g'],\n",
    "                row['Dry_Clover_g'],\n",
    "                row['GDM_g'],\n",
    "                row['Dry_Total_g']\n",
    "            ], dtype=torch.float32)\n",
    "            \n",
    "            return {\n",
    "                'patches': patches,\n",
    "                'metadata': metadata,\n",
    "                'targets': targets,\n",
    "                'sample_id': row['sample_id']\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                'patches': patches,\n",
    "                'metadata': metadata,\n",
    "                'sample_id': row.name # Index for test\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6e481a93",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T09:18:39.548009Z",
     "iopub.status.busy": "2025-12-29T09:18:39.547429Z",
     "iopub.status.idle": "2025-12-29T09:18:39.559188Z",
     "shell.execute_reply": "2025-12-29T09:18:39.558491Z",
     "shell.execute_reply.started": "2025-12-29T09:18:39.547990Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DATA TRANSFORMS\n",
    "# ============================================================================\n",
    "\n",
    "def get_transforms(is_train: bool = True) -> A.Compose:\n",
    "    \"\"\"\n",
    "    Albumentations transforms for image augmentation.\n",
    "    \n",
    "    DINOv2 expects ImageNet normalization.\n",
    "    \"\"\"\n",
    "    if is_train:\n",
    "        return A.Compose([\n",
    "            A.RandomResizedCrop(\n",
    "                size=(IMG_SIZE, IMG_SIZE),\n",
    "                scale=(0.7, 1.0), \n",
    "                ratio=(0.9, 1.1), \n",
    "                p=1.0\n",
    "            ),\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.VerticalFlip(p=0.5),\n",
    "            A.RandomRotate90(p=0.5),\n",
    "            A.Transpose(p=0.5),\n",
    "\n",
    "            A.OneOf([\n",
    "                A.MotionBlur(p=0.2),\n",
    "                A.GaussianBlur(p=0.2),\n",
    "            ], p=0.2),\n",
    "\n",
    "            # Photometric (cautious with hue to avoid confusing green/dead)\n",
    "            A.OneOf([\n",
    "                A.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.05, p=0.3),\n",
    "                A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n",
    "                A.RandomGamma(gamma_limit=(80, 120), p=0.3),\n",
    "                A.HueSaturationValue(p=0.3),\n",
    "            ], p=0.3),\n",
    "\n",
    "            # Noise and regularization\n",
    "            A.GaussNoise(var_limit=(10.0, 50.0), p=0.2),\n",
    "            A.CoarseDropout(max_holes=8, max_height=32, max_width=32, fill_value=0, p=0.3),\n",
    "\n",
    "            # Normalization (DINOv2 uses ImageNet stats)\n",
    "            A.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225]\n",
    "            ),\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "    else:\n",
    "        return A.Compose([\n",
    "            A.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225]\n",
    "            ),\n",
    "            ToTensorV2()\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "43ae7e1b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T09:18:39.560098Z",
     "iopub.status.busy": "2025-12-29T09:18:39.559880Z",
     "iopub.status.idle": "2025-12-29T09:18:39.570268Z",
     "shell.execute_reply": "2025-12-29T09:18:39.569534Z",
     "shell.execute_reply.started": "2025-12-29T09:18:39.560076Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# METRICS\n",
    "# ============================================================================\n",
    "\n",
    "def weighted_r2_score(\n",
    "    y_true: np.ndarray,\n",
    "    y_pred: np.ndarray,\n",
    "    weights: np.ndarray\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Compute weighted R2 score as per competition metric.\n",
    "    \n",
    "    R2_w = 1 - (SS_res_w / SS_tot_w)\n",
    "    \"\"\"\n",
    "    # Weighted mean\n",
    "    y_mean = np.average(y_true, weights=weights)\n",
    "\n",
    "    # Weighted residual sum of squares\n",
    "    ss_res = np.sum(weights * (y_true - y_pred) ** 2)\n",
    "\n",
    "    # Weighted total sum of squares\n",
    "    ss_tot = np.sum(weights * (y_true - y_mean) ** 2)\n",
    "\n",
    "    r2 = 1 - (ss_res / ss_tot + 1e-8)\n",
    "    return r2\n",
    "\n",
    "def compute_metrics(\n",
    "    targets: torch.Tensor,\n",
    "    predictions: torch.Tensor\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Compute all evaluation metrics.\n",
    "    \n",
    "    Args:\n",
    "        targets: (n_samples, 5)\n",
    "        predictions: (n_samples, 5)\n",
    "    \n",
    "    Returns:\n",
    "        Dict of metric names to values\n",
    "    \"\"\"\n",
    "    targets_np = targets.cpu().numpy()\n",
    "    preds_np = predictions.cpu().numpy()\n",
    "\n",
    "    # Per-component MAE\n",
    "    component_mae = {}\n",
    "    for i, name in enumerate(TARGET_NAMES):\n",
    "        mae = np.abs(targets_np[:, i] - preds_np[:, i]).mean()\n",
    "        component_mae[f'{name}_MAE'] = mae\n",
    "\n",
    "    # Weighted R2\n",
    "    # Expand weights to match row-wise structure\n",
    "    n_samples = len(targets_np)\n",
    "    weights_expanded = np.array([TARGET_WEIGHTS[name] for name in TARGET_NAMES])\n",
    "    weights_tiled = np.tile(weights_expanded, n_samples)\n",
    "    \n",
    "    targets_flat = targets_np.flatten()\n",
    "    preds_flat = preds_np.flatten()\n",
    "    \n",
    "    r2_weighted = weighted_r2_score(targets_flat, preds_flat, weights_tiled)\n",
    "    \n",
    "    return {\n",
    "        **component_mae,\n",
    "        'Weighted_R2': r2_weighted\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f29204",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T09:18:39.571530Z",
     "iopub.status.busy": "2025-12-29T09:18:39.571203Z",
     "iopub.status.idle": "2025-12-29T09:18:39.584546Z",
     "shell.execute_reply": "2025-12-29T09:18:39.583950Z",
     "shell.execute_reply.started": "2025-12-29T09:18:39.571509Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# TRAINING LOOP\n",
    "# ============================================================================\n",
    "\n",
    "def train_epoch(\n",
    "    model: AFHN,\n",
    "    dataloader: DataLoader,\n",
    "    optimizer: optim.Optimizer,\n",
    "    device: torch.device,\n",
    "    use_ziln: bool = True\n",
    ") -> float:\n",
    "    \"\"\"Train for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    pbar = tqdm(dataloader, desc='Training')\n",
    "    for batch in pbar:\n",
    "        patches = batch['patches'].to(device)\n",
    "        metadata = batch['metadata'].to(device)\n",
    "        targets = batch['targets'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Get predicted components (final output)\n",
    "        pred_components = model.predict_components(patches, metadata)\n",
    "        \n",
    "        # Component-wise losses\n",
    "        if use_ziln:\n",
    "            # Use ZILN loss for each component\n",
    "            criterion = ZeroInflatedLogNormalLoss()\n",
    "            # Need to get ZILN params for each component\n",
    "            # Since model outputs total×ratios×gates, use MSE instead\n",
    "            loss_components = F.mse_loss(pred_components, targets)\n",
    "        else:\n",
    "            loss_components = F.mse_loss(pred_components, targets)\n",
    "        \n",
    "        # Gate loss (encourage correct zero prediction)\n",
    "        outputs = model(patches, metadata)\n",
    "        gate_targets = (targets > 0).float()\n",
    "        loss_gate = F.binary_cross_entropy_with_logits(\n",
    "            outputs['gates'], gate_targets\n",
    "        )\n",
    "        \n",
    "        # Total biomass consistency loss\n",
    "        pred_total = pred_components.sum(dim=1)\n",
    "        target_total = targets[:, -1]  # Dry_Total_g is last column\n",
    "        loss_total = F.mse_loss(pred_total, target_total)\n",
    "        \n",
    "        # Combined loss\n",
    "        loss = loss_components + 0.1 * loss_gate + 0.2 * loss_total\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "    \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def validate(\n",
    "    model: AFHN,\n",
    "    dataloader: DataLoader,\n",
    "    device: torch.device\n",
    ") -> Tuple[float, Dict[str, float]]:\n",
    "    \"\"\"Validate model.\"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    all_targets = []\n",
    "    all_predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc='Validation'):\n",
    "            patches = batch['patches'].to(device)\n",
    "            metadata = batch['metadata'].to(device)\n",
    "            targets = batch['targets']\n",
    "\n",
    "            predictions = model.predict_components(patches, metadata)\n",
    "\n",
    "            all_targets.append(targets)\n",
    "            all_predictions.append(predictions.cpu())\n",
    "    \n",
    "    all_targets = torch.cat(all_targets)\n",
    "    all_predictions = torch.cat(all_predictions)\n",
    "\n",
    "    metrics = compute_metrics(all_targets, all_predictions)\n",
    "\n",
    "    return metrics['Weighted_R2'], metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2422633d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T09:18:39.585621Z",
     "iopub.status.busy": "2025-12-29T09:18:39.585367Z",
     "iopub.status.idle": "2025-12-29T09:18:39.603456Z",
     "shell.execute_reply": "2025-12-29T09:18:39.602797Z",
     "shell.execute_reply.started": "2025-12-29T09:18:39.585595Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# MAIN TRAINING SCRIPT\n",
    "# ============================================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main training function.\"\"\"\n",
    "    # Load data\n",
    "    print(\"Loading training data...\")\n",
    "    train_df = pd.read_csv(TRAIN_CSV)\n",
    "\n",
    "    # Pivot from long to wide format\n",
    "    train_df_wide = train_df.pivot_table(\n",
    "        index=['sample_id', 'image_path', 'Sampling_Date', 'State',\n",
    "               'Species', 'Pre_GSHH_NDVI', 'Height_Ave_cm'],\n",
    "        columns='target_name',\n",
    "        values='target'\n",
    "    ).reset_index()\n",
    "\n",
    "    # Group K-Fold split (by State to ensure generalization)\n",
    "    groups = train_df_wide['State'].values\n",
    "    n_groups = len(np.unique(groups))\n",
    "    print(f\"Found {n_groups} unique states: {np.unique(groups)}\")\n",
    "    \n",
    "    # Adjust n_splits to not exceed the number of groups\n",
    "    n_splits = 5\n",
    "    if n_groups < n_splits:\n",
    "        print(f\"Warning: Requested {n_splits} splits but only found {n_groups} groups.\")\n",
    "        print(f\"Adjusting n_splits to {n_groups} (Leave-One-Group-Out CV).\")\n",
    "        n_splits = n_groups\n",
    "\n",
    "    # Initialize GroupKFold with the safe number of splits\n",
    "    gkf = GroupKFold(n_splits=n_splits)\n",
    "    \n",
    "    fold = 0\n",
    "    for train_idx, val_idx in gkf.split(train_df_wide, groups=groups):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Fold {fold + 1}/5\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        train_fold = train_df_wide.iloc[train_idx]\n",
    "        val_fold = train_df_wide.iloc[val_idx]\n",
    "        \n",
    "        # Datasets\n",
    "        train_dataset = PatchBiomassDataset(\n",
    "            train_fold, TRAIN_IMG_DIR,\n",
    "            transform=get_transforms(is_train=True)\n",
    "        )\n",
    "        val_dataset = PatchBiomassDataset(\n",
    "            val_fold, TRAIN_IMG_DIR,\n",
    "            transform=get_transforms(is_train=False)\n",
    "        )\n",
    "        \n",
    "        train_loader = DataLoader(\n",
    "            train_dataset, batch_size=BATCH_SIZE,\n",
    "            shuffle=True, num_workers=4, pin_memory=True\n",
    "        )\n",
    "        val_loader = DataLoader(\n",
    "            val_dataset, batch_size=BATCH_SIZE,\n",
    "            shuffle=False, num_workers=4, pin_memory=True\n",
    "        )\n",
    "        \n",
    "        # Model\n",
    "        model = AFHN(num_components=5, meta_dim=8, use_ziln=True).to(DEVICE)\n",
    "        \n",
    "        # Optimizer & Scheduler\n",
    "        optimizer = optim.AdamW(\n",
    "            model.parameters(),\n",
    "            lr=LEARNING_RATE,\n",
    "            weight_decay=WEIGHT_DECAY\n",
    "        )\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "            optimizer, T_0=10, T_mult=2\n",
    "        )\n",
    "        \n",
    "        # Loss functions\n",
    "        criterion_components = ZeroInflatedLogNormalLoss()\n",
    "        criterion_total = TweedieLoss(p=1.5)\n",
    "        \n",
    "        # Training loop\n",
    "        best_r2 = -float('inf')\n",
    "        patience_counter = 0\n",
    "        \n",
    "        for epoch in range(NUM_EPOCHS):\n",
    "            print(f\"\\nEpoch {epoch + 1}/{NUM_EPOCHS}\")\n",
    "            \n",
    "            train_loss = train_epoch(\n",
    "                model, train_loader, optimizer,\n",
    "                criterion_components, criterion_total, DEVICE\n",
    "            )\n",
    "            \n",
    "            val_r2, val_metrics = validate(model, val_loader, DEVICE)\n",
    "            \n",
    "            scheduler.step()\n",
    "            \n",
    "            print(f\"Train Loss: {train_loss:.4f}\")\n",
    "            print(f\"Val Weighted R2: {val_r2:.4f}\")\n",
    "            for k, v in val_metrics.items():\n",
    "                if k != 'Weighted_R2':\n",
    "                    print(f\"  {k}: {v:.4f}\")\n",
    "            \n",
    "            # Early stopping\n",
    "            if val_r2 > best_r2:\n",
    "                best_r2 = val_r2\n",
    "                patience_counter = 0\n",
    "                torch.save(model.state_dict(), f'afhn_fold{fold}_best.pth')\n",
    "                print(f\"Saved best model (R2={best_r2:.4f})\")\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= PATIENCE:\n",
    "                    print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "                    break\n",
    "        \n",
    "        print(f\"\\nFold {fold + 1} Best R2: {best_r2:.4f}\")\n",
    "        fold += 1\n",
    "        \n",
    "        # Only train one fold for demo (remove break for full CV)\n",
    "        break\n",
    "\n",
    "\n",
    "def inference_example():\n",
    "    \"\"\"Example inference on test set.\"\"\"\n",
    "    # Load test data\n",
    "    test_df = pd.read_csv(TEST_CSV)\n",
    "\n",
    "    test_dataset = PatchBiomassDataset(\n",
    "        test_df, TEST_IMG_DIR,\n",
    "        transform=get_transforms(is_train=False),\n",
    "        is_test=True\n",
    "    )\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, batch_size=4,\n",
    "        shuffle=False, num_workers=4\n",
    "    )\n",
    "\n",
    "    # Load model\n",
    "    model = AFHN(num_components=5, meta_dim=8, use_ziln=True).to(DEVICE)\n",
    "    model.load_state_dict(torch.load(MODEL_PATH))\n",
    "\n",
    "    # Predict\n",
    "    predictions = []\n",
    "    sample_ids = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc='Inference'):\n",
    "            patches = batch['patches'].to(DEVICE)\n",
    "            metadata = batch['metadata'].to(DEVICE)\n",
    "            \n",
    "            preds = model.predict_components(patches, metadata)\n",
    "            predictions.append(preds.cpu().numpy())\n",
    "            sample_ids.extend(batch['sample_id'])\n",
    "    \n",
    "    predictions = np.vstack(predictions)\n",
    "\n",
    "    # Create submission\n",
    "    submission_rows = []\n",
    "\n",
    "    for i, img_id in enumerate(sample_ids):\n",
    "        for j, target_name in enumerate(TARGET_NAMES):\n",
    "            submission_rows.append({\n",
    "                'sample_id': f\"{img_id}__{target_name}\",\n",
    "                'target': predictions[i, j]\n",
    "            })\n",
    "    \n",
    "    submission_df = pd.DataFrame(submission_rows)\n",
    "    submission_df.to_csv('submission.csv', index=False)\n",
    "    print(\"Submission saved to submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4167204e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T09:18:39.604879Z",
     "iopub.status.busy": "2025-12-29T09:18:39.604577Z",
     "iopub.status.idle": "2025-12-29T10:38:02.221327Z",
     "shell.execute_reply": "2025-12-29T10:38:02.220342Z",
     "shell.execute_reply.started": "2025-12-29T09:18:39.604850Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data...\n",
      "Found 4 unique states: ['NSW' 'Tas' 'Vic' 'WA']\n",
      "Warning: Requested 5 splits but only found 4 groups.\n",
      "Adjusting n_splits to 4 (Leave-One-Group-Out CV).\n",
      "\n",
      "============================================================\n",
      "Fold 1/5\n",
      "============================================================\n",
      "Loading DINOv2-Large backbone from local storage...\n",
      "\n",
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 274/274 [48:17<00:00, 10.58s/it, loss=nan]\n",
      "Validation: 100%|██████████| 173/173 [30:30<00:00, 10.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: nan\n",
      "Val Weighted R2: nan\n",
      "  Dry_Green_g_MAE: nan\n",
      "  Dry_Dead_g_MAE: nan\n",
      "  Dry_Clover_g_MAE: nan\n",
      "  GDM_g_MAE: nan\n",
      "  Dry_Total_g_MAE: nan\n",
      "\n",
      "Epoch 2/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 2/274 [00:33<1:16:25, 16.86s/it, loss=nan]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_55/2407351814.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# Run training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;31m# inference_example()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_55/1432871469.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nEpoch {epoch + 1}/{NUM_EPOCHS}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m             train_loss = train_epoch(\n\u001b[0m\u001b[1;32m     87\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0mcriterion_components\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion_total\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDEVICE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_55/708415652.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(model, dataloader, optimizer, criterion_components, criterion_total, device)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0mpbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_postfix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Run training\n",
    "    main()\n",
    "    # inference_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670c2d16-37cd-4eb6-9cb8-24056e3939ba",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 14254895,
     "isSourceIdPinned": false,
     "sourceId": 112509,
     "sourceType": "competition"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 986,
     "modelInstanceId": 3327,
     "sourceId": 4535,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31234,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
