{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a78667",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CSIRO Biomass Competition - Insight Engine Runner\n",
    "==================================================\n",
    "Adapted to handle the long-format train.csv structure and extract\n",
    "biomass-specific patterns.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import the InsightEngine (assumes previous code is saved or in same file)\n",
    "# For standalone execution, paste the InsightEngine class code above this\n",
    "\n",
    "def load_and_transform_biomass_data(csv_path):\n",
    "    \"\"\"\n",
    "    Load CSIRO train.csv and transform from long to wide format.\n",
    "    \n",
    "    Args:\n",
    "        csv_path: Path to train.csv\n",
    "        \n",
    "    Returns:\n",
    "        df_wide: DataFrame with one row per image\n",
    "        df_long: Original long-format DataFrame\n",
    "    \"\"\"\n",
    "    print(\"ðŸ“ Loading CSIRO train.csv...\")\n",
    "    df_long = pd.read_csv(csv_path)\n",
    "    \n",
    "    print(f\"   Loaded: {len(df_long)} rows (long format)\")\n",
    "    print(f\"   Unique images: {df_long['image_path'].nunique()}\")\n",
    "    \n",
    "    # Extract image ID from image_path for grouping\n",
    "    df_long['image_id'] = df_long['image_path'].str.extract(r'ID(\\d+)')[0]\n",
    "    \n",
    "    # Pivot: One row per image, columns for each target\n",
    "    print(\"\\nðŸ”„ Transforming to wide format...\")\n",
    "    df_wide = df_long.pivot_table(\n",
    "        index=['image_id', 'image_path', 'Sampling_Date', 'State', 'Species', \n",
    "               'Pre_GSHH_NDVI', 'Height_Ave_cm'],\n",
    "        columns='target_name',\n",
    "        values='target'\n",
    "    ).reset_index()\n",
    "    \n",
    "    # Flatten column names\n",
    "    df_wide.columns.name = None\n",
    "    \n",
    "    print(f\"   Wide format: {len(df_wide)} rows Ã— {len(df_wide.columns)} columns\")\n",
    "    print(f\"   Target columns: {[col for col in df_wide.columns if col.startswith('Dry_') or col == 'GDM_g']}\")\n",
    "    \n",
    "    return df_wide, df_long\n",
    "\n",
    "\n",
    "def engineer_biomass_features(df):\n",
    "    \"\"\"\n",
    "    Create domain-specific features for biomass prediction.\n",
    "    \"\"\"\n",
    "    print(\"\\nðŸ§¬ Engineering biomass-specific features...\")\n",
    "    \n",
    "    df = df.copy()\n",
    "    \n",
    "    # 1. Date features\n",
    "    df['Sampling_Date'] = pd.to_datetime(df['Sampling_Date'])\n",
    "    df['Month'] = df['Sampling_Date'].dt.month\n",
    "    df['Day_of_Year'] = df['Sampling_Date'].dt.dayofyear\n",
    "    df['Season'] = df['Month'].map({\n",
    "        12: 'Summer', 1: 'Summer', 2: 'Summer',\n",
    "        3: 'Autumn', 4: 'Autumn', 5: 'Autumn',\n",
    "        6: 'Winter', 7: 'Winter', 8: 'Winter',\n",
    "        9: 'Spring', 10: 'Spring', 11: 'Spring'\n",
    "    })\n",
    "    \n",
    "    # Cyclical encoding for time\n",
    "    df['Month_Sin'] = np.sin(2 * np.pi * df['Month'] / 12)\n",
    "    df['Month_Cos'] = np.cos(2 * np.pi * df['Month'] / 12)\n",
    "    df['DOY_Sin'] = np.sin(2 * np.pi * df['Day_of_Year'] / 365)\n",
    "    df['DOY_Cos'] = np.cos(2 * np.pi * df['Day_of_Year'] / 365)\n",
    "    \n",
    "    # 2. Biomass ratios (key predictive signals from research)\n",
    "    df['Dead_to_Total_Ratio'] = df['Dry_Dead_g'] / (df['Dry_Total_g'] + 1e-6)\n",
    "    df['Clover_to_Total_Ratio'] = df['Dry_Clover_g'] / (df['Dry_Total_g'] + 1e-6)\n",
    "    df['Green_to_Total_Ratio'] = df['Dry_Green_g'] / (df['Dry_Total_g'] + 1e-6)\n",
    "    df['GDM_to_Total_Ratio'] = df['GDM_g'] / (df['Dry_Total_g'] + 1e-6)\n",
    "    \n",
    "    # 3. NDVI-based features\n",
    "    df['NDVI_x_Height'] = df['Pre_GSHH_NDVI'] * df['Height_Ave_cm']\n",
    "    df['Biomass_per_NDVI'] = df['Dry_Total_g'] / (df['Pre_GSHH_NDVI'] + 0.1)\n",
    "    df['Biomass_per_Height'] = df['Dry_Total_g'] / (df['Height_Ave_cm'] + 1)\n",
    "    \n",
    "    # 4. Binary flags\n",
    "    df['Has_Clover'] = (df['Dry_Clover_g'] > 0).astype(int)\n",
    "    df['Has_Dead'] = (df['Dry_Dead_g'] > 0).astype(int)\n",
    "    df['Is_WA'] = (df['State'] == 'WA').astype(int)\n",
    "    \n",
    "    # 5. Species encoding\n",
    "    df['Species_Count'] = df['Species'].str.count('_') + 1  # Number of species\n",
    "    df['Has_Ryegrass'] = df['Species'].str.contains('Ryegrass', case=False).astype(int)\n",
    "    df['Has_Clover_Species'] = df['Species'].str.contains('Clover|clover', case=False).astype(int)\n",
    "    df['Has_Lucerne'] = df['Species'].str.contains('Lucerne', case=False).astype(int)\n",
    "    \n",
    "    # 6. State encoding (ordinal by typical biomass productivity)\n",
    "    state_map = {'Tas': 0, 'Vic': 1, 'NSW': 2, 'SA': 3, 'WA': 4, 'QLD': 5}\n",
    "    df['State_Code'] = df['State'].map(state_map).fillna(0)\n",
    "    \n",
    "    print(f\"   Created {len([c for c in df.columns if c not in ['image_id', 'image_path', 'Sampling_Date', 'State', 'Species', 'Season']])} numeric features\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def run_biomass_analysis(csv_path='/kaggle/input/csiro-biomass/train.csv', \n",
    "                         output_dir='biomass_insights'):\n",
    "    \"\"\"\n",
    "    Complete analysis pipeline for CSIRO biomass data.\n",
    "    \"\"\"\n",
    "    print(\"=\"*70)\n",
    "    print(\"ðŸŒ¾ CSIRO BIOMASS COMPETITION - AUTOMATED INSIGHT DISCOVERY\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Step 1: Load and transform data\n",
    "    df_wide, df_long = load_and_transform_biomass_data(csv_path)\n",
    "    \n",
    "    # Step 2: Engineer features\n",
    "    df_analysis = engineer_biomass_features(df_wide)\n",
    "    \n",
    "    # Step 3: Define columns for analysis\n",
    "    # Exclude identifiers and categorical text columns\n",
    "    exclude_cols = [\n",
    "        'image_id', 'image_path', 'Sampling_Date', 'State', 'Species', 'Season'\n",
    "    ]\n",
    "    \n",
    "    # Target columns\n",
    "    target_cols = ['Dry_Green_g', 'Dry_Dead_g', 'Dry_Clover_g', 'GDM_g', 'Dry_Total_g']\n",
    "    \n",
    "    # Step 4: Run analysis for each target\n",
    "    all_insights = {}\n",
    "    \n",
    "    for target in target_cols:\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(f\"ðŸŽ¯ ANALYZING TARGET: {target}\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        # Create engine instance\n",
    "        engine = InsightEngine(output_dir=f'{output_dir}/{target}')\n",
    "        \n",
    "        # Run analysis\n",
    "        insights = engine.analyze(\n",
    "            df_analysis,\n",
    "            target_col=target,\n",
    "            id_cols=exclude_cols\n",
    "        )\n",
    "        \n",
    "        # Generate report\n",
    "        engine.generate_report(f'{target}_report.txt')\n",
    "        \n",
    "        all_insights[target] = insights\n",
    "    \n",
    "    # Step 5: Cross-target analysis\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ðŸ”¬ CROSS-TARGET PATTERN ANALYSIS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    analyze_target_relationships(df_analysis, target_cols, output_dir)\n",
    "    \n",
    "    # Step 6: Investigate WA Dead Matter Anomaly\n",
    "    investigate_wa_anomaly(df_analysis, output_dir)\n",
    "    \n",
    "    # Step 7: Zero-inflation analysis\n",
    "    analyze_zero_inflation(df_analysis, target_cols, output_dir)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"âœ… COMPLETE ANALYSIS FINISHED\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\nðŸ“‚ All outputs saved to: {output_dir}/\")\n",
    "    print(f\"\\nðŸ’¡ KEY FINDINGS TO CHECK:\")\n",
    "    print(\"   1. Feature importance rankings for each target\")\n",
    "    print(\"   2. WA Dead Matter investigation results\")\n",
    "    print(\"   3. Zero-inflation patterns\")\n",
    "    print(\"   4. Cluster profiles (species/state groupings)\")\n",
    "    print(\"   5. Interaction features (NDVI Ã— Height, ratios)\")\n",
    "    \n",
    "    return all_insights, df_analysis\n",
    "\n",
    "\n",
    "def analyze_target_relationships(df, target_cols, output_dir):\n",
    "    \"\"\"\n",
    "    Analyze how the five targets relate to each other.\n",
    "    \"\"\"\n",
    "    print(\"\\nðŸ“Š Target Correlation Matrix:\")\n",
    "    target_corr = df[target_cols].corr()\n",
    "    print(target_corr.round(3))\n",
    "    \n",
    "    # Check hierarchical consistency\n",
    "    print(\"\\nðŸ” Checking Hierarchical Consistency:\")\n",
    "    df_check = df.copy()\n",
    "    df_check['GDM_calculated'] = df_check['Dry_Green_g'] + df_check['Dry_Clover_g']\n",
    "    df_check['Total_calculated'] = df_check['GDM_g'] + df_check['Dry_Dead_g']\n",
    "    \n",
    "    gdm_error = (df_check['GDM_g'] - df_check['GDM_calculated']).abs().mean()\n",
    "    total_error = (df_check['Dry_Total_g'] - df_check['Total_calculated']).abs().mean()\n",
    "    \n",
    "    print(f\"   GDM = Green + Clover: MAE = {gdm_error:.4f}g\")\n",
    "    print(f\"   Total = GDM + Dead: MAE = {total_error:.4f}g\")\n",
    "    \n",
    "    if gdm_error > 1.0 or total_error > 1.0:\n",
    "        print(\"   âš ï¸  Warning: Hierarchical constraints not perfectly satisfied!\")\n",
    "    else:\n",
    "        print(\"   âœ… Hierarchical constraints validated\")\n",
    "\n",
    "\n",
    "def investigate_wa_anomaly(df, output_dir):\n",
    "    \"\"\"\n",
    "    Investigate the Western Australia dead biomass anomaly.\n",
    "    \"\"\"\n",
    "    print(\"\\nðŸ”¬ INVESTIGATING WA DEAD MATTER ANOMALY:\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Group by state\n",
    "    state_stats = df.groupby('State').agg({\n",
    "        'Dry_Dead_g': ['mean', 'std', 'min', 'max', lambda x: (x == 0).sum()],\n",
    "        'Dry_Total_g': 'mean',\n",
    "        'Pre_GSHH_NDVI': 'mean',\n",
    "        'Height_Ave_cm': 'mean'\n",
    "    }).round(2)\n",
    "    \n",
    "    print(\"\\nDead Biomass by State:\")\n",
    "    print(state_stats)\n",
    "    \n",
    "    # WA specific analysis\n",
    "    wa_data = df[df['State'] == 'WA']\n",
    "    wa_dead_zero_pct = (wa_data['Dry_Dead_g'] == 0).sum() / len(wa_data) * 100\n",
    "    \n",
    "    print(f\"\\nðŸ“ Western Australia:\")\n",
    "    print(f\"   Samples: {len(wa_data)}\")\n",
    "    print(f\"   Dead = 0: {wa_dead_zero_pct:.1f}% of samples\")\n",
    "    print(f\"   Mean NDVI: {wa_data['Pre_GSHH_NDVI'].mean():.3f}\")\n",
    "    print(f\"   Mean Height: {wa_data['Height_Ave_cm'].mean():.2f} cm\")\n",
    "    \n",
    "    if wa_dead_zero_pct > 80:\n",
    "        print(\"\\n   âš ï¸  CONFIRMED: WA shows systematic zero dead matter\")\n",
    "        print(\"   ðŸ’¡ RECOMMENDATION: Use state-conditional model or post-processing\")\n",
    "        print(\"      â†’ IF State == WA: Set Dry_Dead_g = 0\")\n",
    "\n",
    "\n",
    "def analyze_zero_inflation(df, target_cols, output_dir):\n",
    "    \"\"\"\n",
    "    Analyze zero-inflation patterns in targets.\n",
    "    \"\"\"\n",
    "    print(\"\\nðŸ“‰ ZERO-INFLATION ANALYSIS:\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for target in target_cols:\n",
    "        zero_pct = (df[target] == 0).sum() / len(df) * 100\n",
    "        zero_count = (df[target] == 0).sum()\n",
    "        \n",
    "        # Statistical properties\n",
    "        mean_val = df[target].mean()\n",
    "        std_val = df[target].std()\n",
    "        cv = std_val / (mean_val + 1e-6)  # Coefficient of variation\n",
    "        \n",
    "        # Distribution shape\n",
    "        skewness = df[target].skew()\n",
    "        \n",
    "        print(f\"\\n{target}:\")\n",
    "        print(f\"   Zero values: {zero_count} ({zero_pct:.1f}%)\")\n",
    "        print(f\"   Mean: {mean_val:.2f}g, StdDev: {std_val:.2f}g\")\n",
    "        print(f\"   CV: {cv:.2f}, Skewness: {skewness:.2f}\")\n",
    "        \n",
    "        if zero_pct > 20:\n",
    "            print(f\"   âš ï¸  HIGH zero-inflation detected\")\n",
    "            print(f\"   ðŸ’¡ Recommendation: Use Tweedie Loss (p=1.5) or ZIG model\")\n",
    "        elif zero_pct > 5:\n",
    "            print(f\"   âš¡ MODERATE zero-inflation\")\n",
    "            print(f\"   ðŸ’¡ Recommendation: Consider Tweedie Loss\")\n",
    "        else:\n",
    "            print(f\"   âœ… Low zero-inflation, standard losses OK\")\n",
    "\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Run the complete analysis\n",
    "    insights, df_processed = run_biomass_analysis(\n",
    "        csv_path='/kaggle/input/csiro-biomass/train.csv',\n",
    "        output_dir='biomass_insights'\n",
    "    )\n",
    "    \n",
    "    print(\"\\nðŸŽ‰ Analysis complete! Review the outputs to guide your modeling strategy.\")\n",
    "    print(\"\\nðŸ“‹ NEXT STEPS:\")\n",
    "    print(\"   1. Review feature importance rankings â†’ Focus model on top features\")\n",
    "    print(\"   2. Check cluster profiles â†’ Consider separate models per cluster\")\n",
    "    print(\"   3. Use interaction features â†’ Add to your DINOv2 metadata fusion\")\n",
    "    print(\"   4. Handle WA anomaly â†’ Apply state-conditional post-processing\")\n",
    "    print(\"   5. Select appropriate loss â†’ Tweedie for Clover/Dead, MSE for Total\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
