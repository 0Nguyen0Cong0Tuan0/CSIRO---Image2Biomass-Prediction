{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3fbfbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CSIRO Image2Biomass - Ensemble and Post-Processing\n",
    "Advanced techniques for hierarchical reconciliation and multi-model ensembling\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple\n",
    "import torch\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "class HierarchicalReconciliation:\n",
    "    \"\"\"\n",
    "    Enforce physical constraints: Total = sum(components)\n",
    "    Uses optimization to find minimal adjustment that satisfies constraints\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.target_indices = {\n",
    "            'Dry_Green_g': 0,\n",
    "            'Dry_Dead_g': 1,\n",
    "            'Dry_Clover_g': 2,\n",
    "            'GDM_g': 3,\n",
    "            'Dry_Total_g': 4\n",
    "        }\n",
    "    \n",
    "    def reconcile_predictions(self, predictions: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Reconcile predictions to satisfy:\n",
    "        - GDM_g = Dry_Green_g + Dry_Clover_g\n",
    "        - Dry_Total_g = GDM_g + Dry_Dead_g\n",
    "        \n",
    "        Args:\n",
    "            predictions: (N, 5) array of predictions\n",
    "        \n",
    "        Returns:\n",
    "            reconciled: (N, 5) array with constraints satisfied\n",
    "        \"\"\"\n",
    "        N = predictions.shape[0]\n",
    "        reconciled = np.zeros_like(predictions)\n",
    "\n",
    "        for i in range(N):\n",
    "            reconciled[i] = self._reconcile_single(predictions[i])\n",
    "        \n",
    "        return reconciled\n",
    "    \n",
    "    def _reconcile_single(self, pred: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Reconcile a single sample using ratio-based adjustment\n",
    "        \n",
    "        Strategy:\n",
    "        1. Trust Dry_Total_g prediction (highest weight)\n",
    "        2. Adjust components proportionally to sum to total\n",
    "        \"\"\"\n",
    "        green = pred[0]\n",
    "        dead = pred[1]\n",
    "        clover = pred[2]\n",
    "        gdm_pred = pred[3]\n",
    "        total_pred = pred[4]\n",
    "\n",
    "        # Component sum (should equal total)\n",
    "        component_sum = green + dead + clover\n",
    "\n",
    "        # Ratio-based adjustment\n",
    "        if component_sum > 0:\n",
    "            # Scale components to match total\n",
    "            scale_factor = total_pred / component_sum\n",
    "            green_adj = green * scale_factor\n",
    "            dead_adj = dead * scale_factor\n",
    "            clover_adj = clover * scale_factor\n",
    "        else:\n",
    "            # If all components are zero, distribute total equally\n",
    "            green_adj = total_pred / 3\n",
    "            dead_adj = total_pred / 3\n",
    "            clover_adj = total_pred / 3\n",
    "        \n",
    "        # Calculate GDM from adjusted components\n",
    "        gdm_adj = green_adj + clover_adj\n",
    "        total_adj = gdm_adj + dead_adj\n",
    "\n",
    "        return np.array([green_adj, dead_adj, clover_adj, gdm_adj, total_adj])\n",
    "    \n",
    "class StateBasedPostProcessing:\n",
    "    \"\"\"\n",
    "    Apply state-specific rules based on Western Australia anomaly\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, state_map_path: str = None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            state_map_path: CSV mapping image_path to state (if available in test)\n",
    "        \"\"\"\n",
    "        self.state_map = None\n",
    "        if state_map_path and Path(state_map_path).exists():\n",
    "            self.state_map = pd.read_csv(state_map_path)\n",
    "    \n",
    "    def apply_wa_correction(self,\n",
    "                            predictions: np.ndarray,\n",
    "                            image_paths: List[str]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Apply WA-specific correction: set Dry_Dead_g = 0 for WA samples\n",
    "        \n",
    "        Args:\n",
    "            predictions: (N, 5) array\n",
    "            image_paths: List of image paths\n",
    "        \n",
    "        Returns:\n",
    "            corrected: (N, 5) array\n",
    "        \"\"\"\n",
    "        if self.state_map is None:\n",
    "            print(\"Warning: No state map available for WA correction\")\n",
    "            return predictions\n",
    "        \n",
    "        corrected = predictions.copy()\n",
    "\n",
    "        # Create path to state mapping\n",
    "        path_to_state = dict(zip(self.state_map['image_path'], \n",
    "                                self.state_map['State']))\n",
    "\n",
    "        for i, img_path in enumerate(image_paths):\n",
    "            state = path_to_state.get(img_path, None)\n",
    "            if state == 'WA':\n",
    "                # Set Dry_Dead_g to 0\n",
    "                corrected[i, 1] = 0.0\n",
    "                # Recalculate Dry_Total_g\n",
    "                corrected[i, 4] = corrected[i, 3] + corrected[i, 1]\n",
    "        \n",
    "        return corrected\n",
    "\n",
    "class ZeroClampingStrategy:\n",
    "    \"\"\"\n",
    "    Clamp near-zero predictions to exact zero for sparse components\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 clover_threshold: float = 5.0,\n",
    "                 dead_threshold: float = 5.0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            clover_threshold: Clamp Dry_Clover_g < threshold to 0\n",
    "            dead_threshold: Clamp Dry_Dead_g < threshold to 0\n",
    "        \"\"\"\n",
    "        self.clover_threshold = clover_threshold\n",
    "        self.dead_threshold = dead_threshold\n",
    "    \n",
    "    def apply_clamping(self, predictions: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Apply zero clamping to sparse components\n",
    "        \n",
    "        Args:\n",
    "            predictions: (N, 5) array\n",
    "        \n",
    "        Returns:\n",
    "            clamped: (N, 5) array\n",
    "        \"\"\"\n",
    "        clamped = predictions.copy()\n",
    "\n",
    "        # Clamp clover\n",
    "        clover_mask = clamped[:, 2] < self.clover_threshold\n",
    "        clamped[clover_mask, 2] = 0.0\n",
    "\n",
    "        # Clamp dead\n",
    "        dead_mask = clamped[:, 1] < self.dead_threshold\n",
    "        clamped[dead_mask, 1] = 0.0\n",
    "\n",
    "        # Recalculate GDM and Total\n",
    "        clamped[:, 3] = clamped[:, 0] + clamped[:, 2]  # GDM\n",
    "        clamped[:, 4] = clamped[:, 3] + clamped[:, 1]  # Total\n",
    "        \n",
    "        return clamped\n",
    "\n",
    "class WeightedEnsemble:\n",
    "    \"\"\"\n",
    "    Weighted averaging of multiple model predictions\n",
    "    Supports automatic weight optimization via validation set\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_weights: Dict[str, float] = None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            model_weights: Dict mapping model_name to weight\n",
    "                          If None, uses uniform weights\n",
    "        \"\"\"\n",
    "        self.model_weights = model_weights\n",
    "    \n",
    "    def ensemble_predictions(self, \n",
    "                           model_predictions: Dict[str, np.ndarray]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Ensemble predictions from multiple models\n",
    "        \n",
    "        Args:\n",
    "            model_predictions: Dict mapping model_name to predictions (N, 5)\n",
    "        \n",
    "        Returns:\n",
    "            ensembled: (N, 5) array\n",
    "        \"\"\"\n",
    "        model_names = list(model_predictions.keys())\n",
    "\n",
    "        # Initialize weights\n",
    "        if self.model_weights is None:\n",
    "            weights = {name: 1.0 / len(model_names) for name in model_names}\n",
    "        else:\n",
    "            weights = self.model_weights\n",
    "\n",
    "        # Normalize weights\n",
    "        total_weight = sum(weights.values())\n",
    "        weights = {k: v / total_weight for k, v in weights.items()}\n",
    "\n",
    "        # Weighted average\n",
    "        ensembled = np.zeros_like(model_predictions[model_names[0]])\n",
    "        for name, preds in model_predictions.items():\n",
    "            ensembled += preds * weights[name]\n",
    "        \n",
    "        return ensembled\n",
    "    \n",
    "    def optimize_weights(self, \n",
    "                        model_predictions: Dict[str, np.ndarray],\n",
    "                        targets: np.ndarray,\n",
    "                        competition_weights: np.ndarray) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Optimize ensemble weights using validation targets\n",
    "        \n",
    "        Args:\n",
    "            model_predictions: Dict of model predictions\n",
    "            targets: Ground truth (N, 5)\n",
    "            competition_weights: (5,) array of target weights\n",
    "        \n",
    "        Returns:\n",
    "            optimal_weights: Dict of optimized weights\n",
    "        \"\"\" \n",
    "        model_names = list(model_predictions.keys())\n",
    "        n_models = len(model_names)\n",
    "\n",
    "        def objective(weights):\n",
    "            # Normalize weights\n",
    "            weights = weights / weights.sum()\n",
    "            \n",
    "            # Ensemble\n",
    "            ensembled = np.zeros_like(targets)\n",
    "            for i, name in enumerate(model_names):\n",
    "                ensembled += model_predictions[name] * weights[i]\n",
    "            \n",
    "            # Calculate weighted MSE\n",
    "            errors = (targets - ensembled) ** 2\n",
    "            weighted_errors = errors * competition_weights[np.newaxis, :]\n",
    "            loss = weighted_errors.mean()\n",
    "            \n",
    "            return loss\n",
    "\n",
    "        # Optimize\n",
    "        x0 = np.ones(n_models) / n_models  # Initial uniform weights\n",
    "        bounds = [(0, 1) for _ in range(n_models)]\n",
    "        constraints = {'type': 'eq', 'fun': lambda w: w.sum() - 1}\n",
    "        \n",
    "        result = minimize(objective, x0, bounds=bounds, constraints=constraints,\n",
    "                         method='SLSQP')\n",
    "        \n",
    "        optimal_weights = dict(zip(model_names, result.x))\n",
    "        return optimal_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b91edfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_postprocessing_pipeline(\n",
    "    predictions: np.ndarray,\n",
    "    image_paths: List[str],\n",
    "    apply_reconciliation: bool = True,\n",
    "    apply_wa_correction: bool = False,\n",
    "    apply_clamping: bool = True,\n",
    "    clover_threshold: float = 5.0,\n",
    "    dead_threshold: float = 5.0,\n",
    "    state_map_path: str = None\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Complete post-processing pipeline\n",
    "    \n",
    "    Args:\n",
    "        predictions: (N, 5) raw model predictions\n",
    "        image_paths: List of image paths\n",
    "        apply_reconciliation: Whether to enforce hierarchical constraints\n",
    "        apply_wa_correction: Whether to apply WA dead biomass correction\n",
    "        apply_clamping: Whether to clamp near-zero values\n",
    "        clover_threshold: Threshold for clamping clover\n",
    "        dead_threshold: Threshold for clamping dead\n",
    "        state_map_path: Path to state mapping CSV\n",
    "    \n",
    "    Returns:\n",
    "        processed: (N, 5) post-processed predictions\n",
    "    \"\"\"\n",
    "    processed = predictions.copy()\n",
    "    \n",
    "    # Step 1: Zero clamping (before reconciliation)\n",
    "    if apply_clamping:\n",
    "        print(\"Applying zero clamping...\")\n",
    "        clamper = ZeroClampingStrategy(clover_threshold, dead_threshold)\n",
    "        processed = clamper.apply_clamping(processed)\n",
    "    \n",
    "    # Step 2: State-based correction\n",
    "    if apply_wa_correction:\n",
    "        print(\"Applying WA correction...\")\n",
    "        state_processor = StateBasedPostProcessing(state_map_path)\n",
    "        processed = state_processor.apply_wa_correction(processed, image_paths)\n",
    "    \n",
    "    # Step 3: Hierarchical reconciliation\n",
    "    if apply_reconciliation:\n",
    "        print(\"Applying hierarchical reconciliation...\")\n",
    "        reconciler = HierarchicalReconciliation()\n",
    "        processed = reconciler.reconcile_predictions(processed)\n",
    "    \n",
    "    return processed\n",
    "\n",
    "def create_submission_from_ensemble(\n",
    "    model_paths: List[str],\n",
    "    test_csv_path: str,\n",
    "    output_path: str = \"ensemble_submission.csv\",\n",
    "    model_weights: Dict[str, float] = None\n",
    "):\n",
    "    \"\"\"\n",
    "    Create submission from ensemble of multiple models\n",
    "    \n",
    "    Args:\n",
    "        model_paths: List of paths to trained model checkpoints\n",
    "        test_csv_path: Path to test.csv\n",
    "        output_path: Output submission file path\n",
    "        model_weights: Optional dict of model weights\n",
    "    \"\"\"\n",
    "    from main_training import DinoBiomassModel, BiomassDataset, get_transforms, Config\n",
    "    from torch.utils.data import DataLoader\n",
    "    \n",
    "    config = Config()\n",
    "    \n",
    "    # Load test data\n",
    "    test_df = pd.read_csv(test_csv_path)\n",
    "    test_dataset = BiomassDataset(\n",
    "        test_df,\n",
    "        config.TEST_IMG_DIR,\n",
    "        transform=get_transforms(config.IMG_SIZE, mode='val'),\n",
    "        mode='test'\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=config.BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        num_workers=config.NUM_WORKERS,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    # Load models and generate predictions\n",
    "    model_predictions = {}\n",
    "    image_paths = []\n",
    "    \n",
    "    for i, model_path in enumerate(model_paths):\n",
    "        print(f\"\\nGenerating predictions from model {i+1}/{len(model_paths)}...\")\n",
    "        \n",
    "        # Load model\n",
    "        model = DinoBiomassModel.load_from_checkpoint(model_path, config=config)\n",
    "        model.eval()\n",
    "        model.cuda()\n",
    "        \n",
    "        predictions = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, paths in test_loader:\n",
    "                if i == 0:\n",
    "                    image_paths.extend(paths)\n",
    "                \n",
    "                images = images.cuda()\n",
    "                preds, _ = model(images)\n",
    "                predictions.append(preds.cpu().numpy())\n",
    "        \n",
    "        predictions = np.concatenate(predictions, axis=0)\n",
    "        model_predictions[f\"model_{i}\"] = predictions\n",
    "    \n",
    "    # Ensemble\n",
    "    print(\"\\nEnsembling predictions...\")\n",
    "    ensembler = WeightedEnsemble(model_weights)\n",
    "    ensembled_preds = ensembler.ensemble_predictions(model_predictions)\n",
    "    \n",
    "    # Post-processing\n",
    "    print(\"\\nApplying post-processing...\")\n",
    "    final_preds = full_postprocessing_pipeline(\n",
    "        ensembled_preds,\n",
    "        image_paths,\n",
    "        apply_reconciliation=True,\n",
    "        apply_wa_correction=False,  # Set True if state info available\n",
    "        apply_clamping=True,\n",
    "        clover_threshold=3.0,\n",
    "        dead_threshold=3.0\n",
    "    )\n",
    "    \n",
    "    # Create submission\n",
    "    print(\"\\nCreating submission file...\")\n",
    "    target_names = config.TARGET_NAMES\n",
    "    submission_rows = []\n",
    "    \n",
    "    for idx, img_path in enumerate(image_paths):\n",
    "        img_id = Path(img_path).stem\n",
    "        for i, target_name in enumerate(target_names):\n",
    "            sample_id = f\"{img_id}__{target_name}\"\n",
    "            target_value = final_preds[idx, i]\n",
    "            submission_rows.append({\n",
    "                'sample_id': sample_id,\n",
    "                'target': target_value\n",
    "            })\n",
    "    \n",
    "    submission_df = pd.DataFrame(submission_rows)\n",
    "    submission_df.to_csv(output_path, index=False)\n",
    "    print(f\"\\nSubmission saved to {output_path}\")\n",
    "    \n",
    "    return submission_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43ce276",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "#     # Example usage\n",
    "#     model_paths = [\n",
    "#         \"outputs/fold_0/best-epoch=XX-val_r2=0.XXXX.ckpt\",\n",
    "#         \"outputs/fold_1/best-epoch=XX-val_r2=0.XXXX.ckpt\",\n",
    "#         \"outputs/fold_2/best-epoch=XX-val_r2=0.XXXX.ckpt\",\n",
    "#     ]\n",
    "    \n",
    "#     # Option 1: Uniform ensemble\n",
    "#     create_submission_from_ensemble(\n",
    "#         model_paths,\n",
    "#         \"test.csv\",\n",
    "#         \"ensemble_submission.csv\"\n",
    "#     )\n",
    "    \n",
    "#     # Option 2: Weighted ensemble (weights optimized on validation)\n",
    "#     model_weights = {\n",
    "#         \"model_0\": 0.35,\n",
    "#         \"model_1\": 0.40,\n",
    "#         \"model_2\": 0.25\n",
    "#     }\n",
    "    \n",
    "#     create_submission_from_ensemble(\n",
    "#         model_paths,\n",
    "#         \"test.csv\",\n",
    "#         \"weighted_ensemble_submission.csv\",\n",
    "#         model_weights=model_weights\n",
    "#     )"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
