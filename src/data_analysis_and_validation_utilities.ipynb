{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8785843",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CSIRO Image2Biomass - Tools for EDA, data quality checks, and model validation\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952bc092",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataValidator:\n",
    "    \"\"\"Validate data quality and identify anomalies\"\"\"\n",
    "\n",
    "    def __init__(self, train_csv_path: str):\n",
    "        self.df = pd.read_csv(train_csv_path)\n",
    "        self.df_pivot = self._pivot_to_wide()\n",
    "\n",
    "    def _pivot_to_wide(self) -> pd.DataFrame:\n",
    "        \"\"\"Convert long format to wide format\"\"\"\n",
    "        return self.df.pivot_table(\n",
    "            index=['image_path', 'Sampling_Date', 'State', 'Species', \n",
    "                   'Pre_GSHH_NDVI', 'Height_Ave_cm'],\n",
    "            columns='target_name',\n",
    "            values='target'\n",
    "        ).reset_index()\n",
    "    \n",
    "    def check_hierarchical_consistency(self) -> pd.DataFrame:\n",
    "        \"\"\"Check if GDM = Green + Clover and Total = GDM + Dead\"\"\"\n",
    "        df = self.df_pivot.copy()\n",
    "        \n",
    "        # Calculate expected values\n",
    "        df['GDM_expected'] = df['Dry_Green_g'] + df['Dry_Clover_g']\n",
    "        df['Total_expected'] = df['GDM_g'] + df['Dry_Dead_g']\n",
    "        \n",
    "        # Calculate discrepancies\n",
    "        df['GDM_error'] = np.abs(df['GDM_g'] - df['GDM_expected'])\n",
    "        df['Total_error'] = np.abs(df['Dry_Total_g'] - df['Total_expected'])\n",
    "        \n",
    "        # Flag inconsistent samples\n",
    "        threshold = 1.0  # 1 gram tolerance\n",
    "        inconsistent = df[\n",
    "            (df['GDM_error'] > threshold) | (df['Total_error'] > threshold)\n",
    "        ]\n",
    "        \n",
    "        print(f\"Total samples: {len(df)}\")\n",
    "        print(f\"Inconsistent samples: {len(inconsistent)} ({len(inconsistent)/len(df)*100:.2f}%)\")\n",
    "        print(f\"Mean GDM error: {df['GDM_error'].mean():.3f}g\")\n",
    "        print(f\"Mean Total error: {df['Total_error'].mean():.3f}g\")\n",
    "        \n",
    "        return inconsistent\n",
    "\n",
    "    def analyze_state_distribution(self) -> pd.DataFrame:\n",
    "        \"\"\"Analyze biomass distributions by state\"\"\"\n",
    "        df = self.df_pivot.copy()\n",
    "\n",
    "        # Group by state\n",
    "        state_stats = df.groupby('State').agg({\n",
    "            'Dry_Green_g': ['mean', 'std', 'min', 'max'],\n",
    "            'Dry_Dead_g': ['mean', 'std', 'min', 'max', lambda x: (x == 0).sum()],\n",
    "            'Dry_Clover_g': ['mean', 'std', 'min', 'max', lambda x: (x == 0).sum()],\n",
    "            'Dry_Total_g': ['mean', 'std', 'min', 'max']\n",
    "        })\n",
    "\n",
    "        # Check for WA anomaly\n",
    "        wa_dead = df[df['State'] == 'WA']['Dry_Dead_g']\n",
    "        print('\\nState-wise statistics:')\n",
    "        if len(wa_dead) > 0:\n",
    "            print(f\"\\nWA Dead Biomass Analysis:\")\n",
    "            print(f\"  Mean: {wa_dead.mean():.3f}g\")\n",
    "            print(f\"  Zeros: {(wa_dead == 0).sum()}/{len(wa_dead)} ({(wa_dead == 0).sum()/len(wa_dead)*100:.1f}%)\")\n",
    "        \n",
    "        return state_stats\n",
    "\n",
    "    def analyze_zero_inflation(self) -> Dict[str, float]:\n",
    "        \"\"\"Analyze zero-inflation in each target\"\"\"\n",
    "        df = self.df_pivot.copy()\n",
    "\n",
    "        targets = ['Dry_Green_g', 'Dry_Dead_g', 'Dry_Clover_g', 'GDM_g', 'Dry_Total_g']\n",
    "        zero_ratios = {}\n",
    "\n",
    "        print(\"\\nZero-Inflation Analysis:\")\n",
    "        for target in targets:\n",
    "            n_zeros = (df[target] == 0).sum()\n",
    "            ratio = n_zeros / len(df) * 100\n",
    "            zero_ratios[target] = ratio\n",
    "            print(f\"  {target}: {n_zeros}/{len(df)} ({ratio:.2f}% zeros)\")\n",
    "            \n",
    "            # Additional stats for non-zero values\n",
    "            non_zero = df[df[target] > 0][target]\n",
    "            if len(non_zero) > 0:\n",
    "                print(f\"Non-zero: mean={non_zero.mean():.2f}, std={non_zero.std():.2f}, cv={non_zero.std()/non_zero.mean():.2f}\")\n",
    "        \n",
    "        return zero_ratios\n",
    "    \n",
    "    def check_correlation_with_metadata(self):\n",
    "        \"\"\"Analyze correlation between targets and metadata\"\"\"\n",
    "        df = self.df_pivot.copy()\n",
    "        \n",
    "        # Select numeric columns\n",
    "        numeric_cols = ['Pre_GSHH_NDVI', 'Height_Ave_cm', \n",
    "                       'Dry_Green_g', 'Dry_Dead_g', 'Dry_Clover_g', \n",
    "                       'GDM_g', 'Dry_Total_g']\n",
    "        \n",
    "        # Remove NaN values\n",
    "        df_clean = df[numeric_cols].dropna()\n",
    "        \n",
    "        # Calculate correlation\n",
    "        corr_matrix = df_clean.corr()\n",
    "        \n",
    "        # Plot heatmap\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', \n",
    "                   center=0, square=True, linewidths=1)\n",
    "        plt.title('Correlation Matrix: Metadata vs Targets')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('correlation_matrix.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        print(\"\\nCorrelation Analysis:\")\n",
    "        print(\"NDVI correlations:\")\n",
    "        print(corr_matrix['Pre_GSHH_NDVI'].sort_values(ascending=False))\n",
    "        print(\"\\nHeight correlations:\")\n",
    "        print(corr_matrix['Height_Ave_cm'].sort_values(ascending=False))\n",
    "        \n",
    "        return corr_matrix\n",
    "    \n",
    "    def detect_outliers(self, n_std: float = 3.0) -> pd.DataFrame:\n",
    "        \"\"\"Detect outliers using z-score method\"\"\"\n",
    "        df = self.df_pivot.copy()\n",
    "        \n",
    "        targets = ['Dry_Green_g', 'Dry_Dead_g', 'Dry_Clover_g', 'GDM_g', 'Dry_Total_g']\n",
    "        outliers = []\n",
    "        \n",
    "        for target in targets:\n",
    "            mean = df[target].mean()\n",
    "            std = df[target].std()\n",
    "            z_scores = np.abs((df[target] - mean) / std)\n",
    "            \n",
    "            target_outliers = df[z_scores > n_std]\n",
    "            outliers.append({\n",
    "                'target': target,\n",
    "                'n_outliers': len(target_outliers),\n",
    "                'outlier_indices': target_outliers.index.tolist()\n",
    "            })\n",
    "            \n",
    "            print(f\"{target}: {len(target_outliers)} outliers (>{n_std}σ)\")\n",
    "        \n",
    "        return pd.DataFrame(outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd069338",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictionAnalyzer:\n",
    "    \"\"\"Analyze model predictions and calculate metrics\"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 predictions: np.ndarray, \n",
    "                 targets: np.ndarray,\n",
    "                 competition_weights: np.ndarray):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            predictions: (N, 5) array of predictions\n",
    "            targets: (N, 5) array of ground truth\n",
    "            competition_weights: (5,) array of target weights\n",
    "        \"\"\"\n",
    "        self.predictions = predictions\n",
    "        self.targets = targets\n",
    "        self.weights = competition_weights\n",
    "        self.target_names = ['Dry_Green_g', 'Dry_Dead_g', 'Dry_Clover_g', 'GDM_g', 'Dry_Total_g']\n",
    "    \n",
    "    def calculate_r2_score(self) -> Dict[str, float]:\n",
    "        \"\"\"Caculate R2 score for each target and weighted average\"\"\"\n",
    "        results = {}\n",
    "\n",
    "        # Pre-target R2 scores\n",
    "        for i, name in enumerate(self.target_names):\n",
    "            y_true = self.targets[:, i]\n",
    "            y_pred = self.predictions[:, i]\n",
    "            \n",
    "            ss_res = np.sum((y_true - y_pred) ** 2)\n",
    "            ss_tot = np.sum((y_true - y_true.mean()) ** 2)\n",
    "            r2 = 1 - (ss_res / (ss_tot + 1e-8))\n",
    "            \n",
    "            results[f'r2_{name}'] = r2\n",
    "        \n",
    "        # Weighted global R2\n",
    "        pred_flat = self.predictions.flatten()\n",
    "        target_flat = self.targets.flatten()\n",
    "        weights_expanded = np.repeat(self.weights, len(self.targets))\n",
    "        \n",
    "        weighted_mean = np.sum(weights_expanded * target_flat) / np.sum(weights_expanded)\n",
    "        ss_res = np.sum(weights_expanded * (target_flat - pred_flat) ** 2)\n",
    "        ss_tot = np.sum(weights_expanded * (target_flat - weighted_mean) ** 2)\n",
    "        \n",
    "        results['r2_weighted_global'] = 1 - (ss_res / (ss_tot + 1e-8))\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def calculate_mae_rmse(self) -> Dict[str, float]:\n",
    "        \"\"\"Calculate MAE and RMSE for each target\"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        for i, name in enumerate(self.target_names):\n",
    "            y_true = self.targets[:, i]\n",
    "            y_pred = self.predictions[:, i]\n",
    "            \n",
    "            mae = np.mean(np.abs(y_true - y_pred))\n",
    "            rmse = np.sqrt(np.mean((y_true - y_pred) ** 2))\n",
    "            \n",
    "            results[f'mae_{name}'] = mae\n",
    "            results[f'rmse_{name}'] = rmse\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def plot_predictions_vs_actual(self, save_path: str = 'pred_vs_actual.png'):\n",
    "        \"\"\"Create scatter plots of predictions vs actual values\"\"\"\n",
    "        fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        for i, name in enumerate(self.target_names):\n",
    "            ax = axes[i]\n",
    "            y_true = self.targets[:, i]\n",
    "            y_pred = self.predictions[:, i]\n",
    "            \n",
    "            # Scatter plot\n",
    "            ax.scatter(y_true, y_pred, alpha=0.5, s=20)\n",
    "            \n",
    "            # Ideal line\n",
    "            max_val = max(y_true.max(), y_pred.max())\n",
    "            ax.plot([0, max_val], [0, max_val], 'r--', lw=2, label='Ideal')\n",
    "            \n",
    "            # Calculate R2\n",
    "            ss_res = np.sum((y_true - y_pred) ** 2)\n",
    "            ss_tot = np.sum((y_true - y_true.mean()) ** 2)\n",
    "            r2 = 1 - (ss_res / (ss_tot + 1e-8))\n",
    "            \n",
    "            ax.set_xlabel('Actual (g)', fontsize=10)\n",
    "            ax.set_ylabel('Predicted (g)', fontsize=10)\n",
    "            ax.set_title(f'{name}\\nR² = {r2:.4f}', fontsize=11)\n",
    "            ax.legend()\n",
    "            ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Remove extra subplot\n",
    "        fig.delaxes(axes[5])\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(f\"Plot saved to {save_path}\")\n",
    "\n",
    "    def analyze_residuals(self) -> pd.DataFrame:\n",
    "        \"\"\"Analyze prediction residuals\"\"\"\n",
    "        residuals = self.targets - self.predictions\n",
    "        \n",
    "        results = []\n",
    "        for i, name in enumerate(self.target_names):\n",
    "            res = residuals[:, i]\n",
    "            results.append({\n",
    "                'target': name,\n",
    "                'mean_residual': res.mean(),\n",
    "                'std_residual': res.std(),\n",
    "                'max_abs_residual': np.abs(res).max(),\n",
    "                'median_abs_residual': np.median(np.abs(res))\n",
    "            })\n",
    "        \n",
    "        return pd.DataFrame(results)\n",
    "    \n",
    "def comprehensive_validation_report(\n",
    "    train_csv_path: str,\n",
    "    predictions_path: str = None,\n",
    "    targets_path: str = None\n",
    "):\n",
    "    \"\"\"Generate comprehensive validation report\"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"CSIRO Biomass Prediction - Validation Report\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Data validation\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"1. DATA QUALITY VALIDATION\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    validator = DataValidator(train_csv_path)\n",
    "    \n",
    "    # Check hierarchical consistency\n",
    "    print(\"\\n--- Hierarchical Consistency ---\")\n",
    "    inconsistent = validator.check_hierarchical_consistency()\n",
    "    \n",
    "    # State distributions\n",
    "    print(\"\\n--- State Distributions ---\")\n",
    "    state_stats = validator.analyze_state_distributions()\n",
    "    \n",
    "    # Zero inflation\n",
    "    print(\"\\n--- Zero Inflation ---\")\n",
    "    zero_ratios = validator.analyze_zero_inflation()\n",
    "    \n",
    "    # Correlation analysis\n",
    "    print(\"\\n--- Correlation Analysis ---\")\n",
    "    corr_matrix = validator.check_correlation_with_metadata()\n",
    "    \n",
    "    # Outlier detection\n",
    "    print(\"\\n--- Outlier Detection ---\")\n",
    "    outliers = validator.detect_outliers(n_std=3.0)\n",
    "    \n",
    "    # Model predictions analysis (if provided)\n",
    "    if predictions_path and targets_path:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"2. PREDICTION ANALYSIS\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        predictions = np.load(predictions_path)\n",
    "        targets = np.load(targets_path)\n",
    "        weights = np.array([0.1, 0.1, 0.1, 0.2, 0.5])\n",
    "        \n",
    "        analyzer = PredictionAnalyzer(predictions, targets, weights)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        print(\"\\n--- R² Scores ---\")\n",
    "        r2_scores = analyzer.calculate_r2_score()\n",
    "        for metric, value in r2_scores.items():\n",
    "            print(f\"{metric}: {value:.4f}\")\n",
    "        \n",
    "        print(\"\\n--- MAE/RMSE ---\")\n",
    "        mae_rmse = analyzer.calculate_mae_rmse()\n",
    "        for metric, value in mae_rmse.items():\n",
    "            print(f\"{metric}: {value:.3f}\")\n",
    "        \n",
    "        # Residual analysis\n",
    "        print(\"\\n--- Residual Analysis ---\")\n",
    "        residuals = analyzer.analyze_residuals()\n",
    "        print(residuals.to_string(index=False))\n",
    "        \n",
    "        # Generate plots\n",
    "        print(\"\\n--- Generating Plots ---\")\n",
    "        analyzer.plot_predictions_vs_actual()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Validation report complete!\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage\n",
    "    comprehensive_validation_report(\n",
    "        train_csv_path=\"train.csv\",\n",
    "        # predictions_path=\"val_predictions.npy\",  # Uncomment if available\n",
    "        # targets_path=\"val_targets.npy\"           # Uncomment if available\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31563ff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions (PIL): 2000x1000\n",
      "Mode: RGB\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "\n",
    "image_input = Path(\"D:/Learn/Kaggle Competition/CSIRO---Image2Biomass-Prediction/data/train/ID2131261930.jpg\")\n",
    "\n",
    "# Open the image\n",
    "with Image.open(image_input) as img:\n",
    "    # img.size returns (width, height)\n",
    "    width, height = img.size\n",
    "    print(f\"Dimensions (PIL): {width}x{height}\")\n",
    "    print(f\"Mode: {img.mode}\") # e.g., 'RGB', 'L' (grayscale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f05d5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from pathlib import Path\n",
    "\n",
    "image_input = Path(\"D:/Learn/Kaggle Competition/CSIRO---Image2Biomass-Prediction/data/train/ID4464212.jpg\")\n",
    "\n",
    "# Read the image (cv2 reads as numpy array)\n",
    "# Note: str() is needed because cv2.imread doesn't always accept Path objects directly in older versions\n",
    "img = cv2.imread(str(image_input))\n",
    "\n",
    "if img is not None:\n",
    "    # img.shape returns (height, width, channels)\n",
    "    h, w, c = img.shape\n",
    "    print(f\"Shape (OpenCV): Height={h}, Width={w}, Channels={c}\")\n",
    "else:\n",
    "    print(\"Could not load image. Check the path.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
