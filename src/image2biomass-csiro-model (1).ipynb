{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Main Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-12-29T05:29:16.776145Z",
     "iopub.status.busy": "2025-12-29T05:29:16.775868Z",
     "iopub.status.idle": "2025-12-29T05:30:05.210885Z",
     "shell.execute_reply": "2025-12-29T05:30:05.210089Z",
     "shell.execute_reply.started": "2025-12-29T05:29:16.776120Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CSIRO Image2Biomass Prediction - Complete Solution\n",
    "Main training script implementing DINOv2, Tweedie Loss, and Hierarchical Constraints\n",
    "\"\"\"\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pytorch_lightning as pl # PyTorch Lightning for training loops used by DINOv2\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, LearningRateMonitor # Callbacks for training monitoring and checkpointing the model\n",
    "from pytorch_lightning.loggers import WandbLogger # Weights & Biases logger for experiment tracking\n",
    "\n",
    "import albumentations as A # Albumentations for data augmentation utilities used in training\n",
    "from albumentations.pytorch import ToTensorV2 # Convert images to PyTorch tensors that can be used in DataLoader\n",
    "from PIL import Image # Python Imaging Library for image processing tasks\n",
    "from sklearn.model_selection import GroupKFold # Group K-Fold cross-validation for splitting dataset into training and validation sets\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "class Config:\n",
    "    # Paths\n",
    "    TRAIN_CSV = \"/kaggle/input/csiro-biomass/train.csv\"\n",
    "    TEST_CSV = \"/kaggle/input/csiro-biomass/test.csv\"\n",
    "    TRAIN_IMG_DIR = \"/kaggle/input/csiro-biomass/train\"\n",
    "    TEST_IMG_DIR = \"/kaggle/input/csiro-biomass\"\n",
    "    OUTPUT_DIR = \"outputs\"\n",
    "\n",
    "    # Model\n",
    "    BACKBONE = \"/kaggle/input/dinov2/pytorch/large/1\"\n",
    "    IMG_SIZE = 518 # DINOv2 optimal input size \n",
    "    NUM_COMPONENTS = 5 # Number of biomass components to predict\n",
    "    HIDDEN_DIM = 512 # Hidden dimension for regression head\n",
    "    DROPOUT = 0.3 # Dropout rate for regression head\n",
    "\n",
    "    # Training\n",
    "    BATCH_SIZE = 16\n",
    "    NUM_WORKERS = 4\n",
    "    MAX_EPOCHS = 10\n",
    "    LEARNING_RATE = 1e-4 \n",
    "    WEIGHT_DECAY = 1e-3 # Weight decay for optimizer\n",
    "    TWEEDIE_P = 1.7 # Tweedie loss power parameter\n",
    "\n",
    "    # Cross-validation\n",
    "    N_FOLDS = 5 # Number of folds for cross-validation\n",
    "    FOLD_TO_TRAIN = 0 # Specify which fold to train on\n",
    "\n",
    "    # Competition weights\n",
    "    TARGET_WEIGHTS = {\n",
    "        'Dry_Green_g': 0.1,\n",
    "        'Dry_Dead_g': 0.1,\n",
    "        'Dry_Clover_g': 0.1,\n",
    "        'GDM_g': 0.2,\n",
    "        'Dry_Total_g': 0.5\n",
    "    }\n",
    "\n",
    "    TARGET_NAMES = ['Dry_Green_g', 'Dry_Dead_g', 'Dry_Clover_g', 'GDM_g', 'Dry_Total_g']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T05:30:05.213226Z",
     "iopub.status.busy": "2025-12-29T05:30:05.212563Z",
     "iopub.status.idle": "2025-12-29T05:30:05.227844Z",
     "shell.execute_reply": "2025-12-29T05:30:05.227057Z",
     "shell.execute_reply.started": "2025-12-29T05:30:05.213192Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DATA PREPROCESSING\n",
    "# ============================================================================\n",
    "\n",
    "def get_transforms(img_size: int, mode: str = 'train'):\n",
    "    \"\"\"Advanced augmentation pipeline for vegetation imagery\"\"\"\n",
    "\n",
    "    if mode == 'train':\n",
    "        return A.Compose([\n",
    "            A.RandomResizedCrop(\n",
    "                size=(img_size, img_size),\n",
    "                scale=(0.7, 1.0), \n",
    "                ratio=(0.9, 1.1), \n",
    "                p=1.0\n",
    "            ),\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.VerticalFlip(p=0.5),\n",
    "            A.RandomRotate90(p=0.5),\n",
    "            A.Transpose(p=0.5),\n",
    "\n",
    "            # Photometric (cautious with hue to avoid confusing green/dead)\n",
    "            A.RandomBrightnessContrast(\n",
    "                brightness_limit=0.2, \n",
    "                contrast_limit=0.2, \n",
    "                p=0.5\n",
    "            ),\n",
    "            A.RandomGamma(\n",
    "                gamma_limit=(80, 120), \n",
    "                p=0.3\n",
    "            ),\n",
    "            A.HueSaturationValue(\n",
    "                hue_shift_limit=10, \n",
    "                sat_shift_limit=20, \n",
    "                val_shift_limit=20, \n",
    "                p=0.3\n",
    "            ),\n",
    "\n",
    "            # Noise and regularization\n",
    "            A.GaussNoise(var_limit=(10.0, 50.0), p=0.2),\n",
    "            A.CoarseDropout(\n",
    "                max_holes=8,\n",
    "                max_height=32,\n",
    "                max_width=32,\n",
    "                fill_value=0,\n",
    "                p=0.3\n",
    "            ),\n",
    "\n",
    "            # Normalization (DINOv2 uses ImageNet stats)\n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "    else:\n",
    "        return A.Compose([\n",
    "            A.Resize(height=img_size, width=img_size),\n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "\n",
    "class BiomassDataset(Dataset):\n",
    "    \"\"\"Dataset for CSIRO biomass prediction with pivoting from long to wide format\"\"\"\n",
    "\n",
    "    def __init__(self, df: pd.DataFrame, img_dir: str, transform=None, mode: str = 'train'):\n",
    "        self.img_dir = Path(img_dir)\n",
    "        self.transform = transform\n",
    "        self.mode = mode\n",
    "\n",
    "        if mode == 'train':\n",
    "            # Pivot from long to wide format\n",
    "            self.df = df.pivot_table(\n",
    "                index=['image_path', 'Sampling_Date', 'State', 'Species', \n",
    "                       'Pre_GSHH_NDVI', 'Height_Ave_cm'],\n",
    "                columns='target_name',\n",
    "                values='target'\n",
    "            ).reset_index()\n",
    "        \n",
    "            # Metadata normalization parameters\n",
    "            self.df['Height_Ave_cm'] = self.df['Height_Ave_cm'].fillna(0)\n",
    "            self.height_mean = self.df['Height_Ave_cm'].mean()\n",
    "            self.height_std = self.df['Height_Ave_cm'].std()\n",
    "            \n",
    "            self.df['Pre_GSHH_NDVI'] = self.df['Pre_GSHH_NDVI'].fillna(0)\n",
    "            self.ndvi_mean = self.df['Pre_GSHH_NDVI'].mean()\n",
    "            self.ndvi_std = self.df['Pre_GSHH_NDVI'].std()\n",
    "        else:\n",
    "            # Test set: drop duplicates, keep unique images\n",
    "            self.df = df.drop_duplicates(subset=['image_path']).reset_index(drop=True)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "\n",
    "        # Load image\n",
    "        img_path = self.img_dir / row['image_path']\n",
    "        try:\n",
    "            image = np.array(Image.open(img_path).convert('RGB'))\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {img_path}: {e}\")\n",
    "            image = np.zeros((Config.IMG_SIZE, Config.IMG_SIZE, 3), dtype=np.uint8)\n",
    "        \n",
    "        # Apply transformations\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image)\n",
    "            image = augmented['image']\n",
    "        \n",
    "        if self.mode == 'train':\n",
    "            # Extract targets in correct order\n",
    "            targets = torch.tensor([\n",
    "                row['Dry_Green_g'],\n",
    "                row['Dry_Dead_g'],\n",
    "                row['Dry_Clover_g'],\n",
    "                row['GDM_g'],\n",
    "                row['Dry_Total_g']\n",
    "            ], dtype=torch.float32)\n",
    "\n",
    "            # Metadata features\n",
    "            date = pd.to_datetime(row['Sampling_Date'])\n",
    "            day_of_year = date.dayofyear\n",
    "            sin_date = np.sin(2 * np.pi * day_of_year / 365.0)\n",
    "            cos_date = np.cos(2 * np.pi * day_of_year / 365.0)\n",
    "\n",
    "            height_norm = (row['Height_Ave_cm'] - self.height_mean) / (self.height_std + 1e-6)\n",
    "            ndvi_norm = (row['Pre_GSHH_NDVI'] - self.ndvi_mean) / (self.ndvi_std + 1e-6)\n",
    "\n",
    "            meta = torch.tensor([sin_date, cos_date, height_norm, ndvi_norm], \n",
    "                               dtype=torch.float32)\n",
    "            \n",
    "            return image, meta, targets\n",
    "        else:\n",
    "            return image, str(row['image_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T05:30:05.229076Z",
     "iopub.status.busy": "2025-12-29T05:30:05.228788Z",
     "iopub.status.idle": "2025-12-29T05:30:05.289783Z",
     "shell.execute_reply": "2025-12-29T05:30:05.288969Z",
     "shell.execute_reply.started": "2025-12-29T05:30:05.229039Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# MODEL COMPONENTS\n",
    "# ============================================================================\n",
    "class TweedieLoss(nn.Module):\n",
    "    \"\"\"Tweedie Loss for zero-inflated continuous data (1 < p < 2)\"\"\"\n",
    "\n",
    "    def __init__(self, p: float = 1.5, epsilon: float = 1e-8):\n",
    "        super().__init__()\n",
    "        assert 1 < p < 2, \"Tweedie power p must be in (1, 2)\"\n",
    "        self.p = p\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def forward(self, pred: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
    "        pred = pred + self.epsilon  # Ensure positivity\n",
    "        \n",
    "        # Tweedie deviance\n",
    "        term1 = -target * torch.pow(pred, 1 - self.p) / (1 - self.p)\n",
    "        term2 = torch.pow(pred, 2 - self.p) / (2 - self.p)\n",
    "        \n",
    "        loss = term1 + term2\n",
    "        return loss.mean()\n",
    "\n",
    "class HierarchicalBiomassHead(nn.Module):    \n",
    "    \"\"\"\n",
    "    Ratio-constrained regression head ensuring sum(components) = total\n",
    "    Predicts total biomass + component ratios, then multiplies to get components\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_dim: int, num_components: int = 5, hidden_dim: int = 512):\n",
    "        super().__init__()\n",
    "        self.num_components = num_components\n",
    "\n",
    "        # Common feature extractor\n",
    "        self.common = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        \n",
    "        # Total biomass branch (scalar)\n",
    "        self.total_head = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, 256),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Softplus()  # Ensure positivity\n",
    "        )\n",
    "        \n",
    "        # Component ratio branch (vector)\n",
    "        self.ratio_head = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, 256),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, num_components)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        feat = self.common(x) # Shared feature extraction\n",
    "\n",
    "        # Predict total biomass\n",
    "        total_biomass = self.total_head(feat) # (batch, 1)\n",
    "\n",
    "        # Predict component ratios\n",
    "        ratio_logits = self.ratio_head(feat) # (batch, num_components) \n",
    "        ratios = F.softmax(ratio_logits, dim=1) # (batch, num_components), sum to 1\n",
    "\n",
    "        # Derive component biomass\n",
    "        component_biomass = total_biomass * ratios # (batch, num_components)\n",
    "\n",
    "        return component_biomass, total_biomass, ratios\n",
    "    \n",
    "class DinoBiomassModel(pl.LightningModule):\n",
    "    \"\"\"Main model with DINOv2 backbone and hierarchical constraints\"\"\"\n",
    "\n",
    "    def __init__(self, config: Config = None, **kwargs):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Handle both Config object and individual parameters\n",
    "        if config is not None:\n",
    "            self.config = config\n",
    "            # Training mode: save config as individual hyperparameters\n",
    "            self.save_hyperparameters({\n",
    "                'backbone': config.BACKBONE,\n",
    "                'num_components': config.NUM_COMPONENTS,\n",
    "                'hidden_dim': config.HIDDEN_DIM,\n",
    "                'learning_rate': config.LEARNING_RATE,\n",
    "                'weight_decay': config.WEIGHT_DECAY,\n",
    "                'tweedie_p': config.TWEEDIE_P,\n",
    "                'target_weights': config.TARGET_WEIGHTS\n",
    "            })\n",
    "        else:\n",
    "            # Loading from checkpoint: hyperparameters already loaded\n",
    "            self.save_hyperparameters()\n",
    "        \n",
    "        # Load DINOv2 backbone\n",
    "        self.backbone = timm.create_model(\n",
    "            self.hparams.backbone, \n",
    "            pretrained=True, \n",
    "            num_classes=0  # Remove classification head\n",
    "        )\n",
    "        self.embed_dim = self.backbone.num_features\n",
    "        \n",
    "        # Freeze early layers (layer-wise freezing strategy)\n",
    "        self._freeze_early_layers()\n",
    "        \n",
    "        # Hierarchical regression head\n",
    "        self.head = HierarchicalBiomassHead(\n",
    "            in_dim=self.embed_dim,\n",
    "            num_components=self.hparams.num_components,\n",
    "            hidden_dim=self.hparams.hidden_dim\n",
    "        )\n",
    "        \n",
    "        # Loss function\n",
    "        self.criterion = TweedieLoss(p=self.hparams.tweedie_p)\n",
    "        \n",
    "        # Competition weights\n",
    "        target_weights = self.hparams.target_weights\n",
    "        self.register_buffer('weights', torch.tensor([\n",
    "            target_weights['Dry_Green_g'],\n",
    "            target_weights['Dry_Dead_g'],\n",
    "            target_weights['Dry_Clover_g'],\n",
    "            target_weights['GDM_g'],\n",
    "            target_weights['Dry_Total_g']\n",
    "        ]))\n",
    "        \n",
    "        # Metrics storage\n",
    "        self.validation_step_outputs = []\n",
    "    \n",
    "    def _freeze_early_layers(self):\n",
    "        \"\"\"Freeze patch embedding and early transformer blocks\"\"\"\n",
    "        \n",
    "        # Freeze all parameters first\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Unfreeze last 2-3 blocks for fine-tuning\n",
    "        for name, param in self.backbone.named_parameters():\n",
    "            # For ViT-Small (12 blocks), unfreeze blocks 10-11\n",
    "            if any(x in name for x in ['blocks.10', 'blocks.11', 'norm']):\n",
    "                param.requires_grad = True\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        # DINOv2 feature extraction\n",
    "        features = self.backbone(x)  # (batch, embed_dim)\n",
    "\n",
    "        # Hierarchical regression head\n",
    "        components, total_pred, ratios = self.head(features)  # (batch, num_components), (batch, 1), (batch, num_components)\n",
    "\n",
    "        return components, total_pred\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        images, meta, targets = batch\n",
    "\n",
    "        # Forward pass\n",
    "        preds, total_pred = self(images)\n",
    "\n",
    "        # Weighted Tweedie loss\n",
    "        loss = 0\n",
    "        for i in range(self.config.NUM_COMPONENTS):\n",
    "            component_loss = self.criterion(preds[:, i], targets[:, i])\n",
    "            loss += component_loss * self.weights[i]\n",
    "        \n",
    "        # Logging\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        images, meta, targets = batch\n",
    "        preds, total_pred = self(images)\n",
    "\n",
    "        # Weighted Tweedie loss\n",
    "        loss = 0\n",
    "        for i in range(self.config.NUM_COMPONENTS):\n",
    "            component_loss = self.criterion(preds[:, i], targets[:, i])\n",
    "            loss += component_loss * self.weights[i]\n",
    "        \n",
    "        # MSE for monitoring\n",
    "        mse_loss = F.mse_loss(preds, targets)\n",
    "        \n",
    "        self.log('val_loss', loss, prog_bar=True, on_epoch=True)\n",
    "        self.log('val_mse', mse_loss, prog_bar=True, on_epoch=True)\n",
    "        \n",
    "        # Store predictions for R² calculation\n",
    "        self.validation_step_outputs.append({\n",
    "            'preds': preds.detach().cpu(),\n",
    "            'targets': targets.detach().cpu()\n",
    "        })\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        \"\"\"Calculate weighted R² at end of validation epoch\"\"\"\n",
    "        all_preds = torch.cat([x['preds'] for x in self.validation_step_outputs], dim=0)\n",
    "        all_targets = torch.cat([x['targets'] for x in self.validation_step_outputs], dim=0)\n",
    "        \n",
    "        # Calculate weighted R2\n",
    "        r2_score = self._calculate_weighted_r2(all_preds, all_targets)\n",
    "        self.log('val_r2', r2_score, prog_bar=True)\n",
    "        \n",
    "        self.validation_step_outputs.clear()\n",
    "    \n",
    "    def _calculate_weighted_r2(self, preds: torch.Tensor, targets: torch.Tensor) -> float:\n",
    "        \"\"\"Calculate globally weighted R² as per competition metric\"\"\"\n",
    "        # Flatten and apply weights\n",
    "        preds_flat = preds.view(-1)\n",
    "        targets_flat = targets.view(-1)\n",
    "        weights_expanded = self.weights.cpu().repeat(preds.size(0))\n",
    "\n",
    "        # Weighted mean\n",
    "        weighted_mean = (weights_expanded * targets_flat).sum() / weights_expanded.sum()\n",
    "\n",
    "        # Weighted SS_res and SS_tot\n",
    "        ss_res = (weights_expanded * (targets_flat - preds_flat) ** 2).sum()\n",
    "        ss_tot = (weights_expanded * (targets_flat - weighted_mean) ** 2).sum()\n",
    "        \n",
    "        r2 = 1 - (ss_res / (ss_tot + 1e-8))\n",
    "        return r2.item()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            filter(lambda p: p.requires_grad, self.parameters()),\n",
    "            lr=self.hparams.learning_rate,\n",
    "            weight_decay=self.hparams.weight_decay\n",
    "        )\n",
    "        \n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "            optimizer, T_0=10, T_mult=2, eta_min=1e-6\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'optimizer': optimizer,\n",
    "            'lr_scheduler': {\n",
    "                'scheduler': scheduler,\n",
    "                'monitor': 'val_loss'\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T05:30:05.290956Z",
     "iopub.status.busy": "2025-12-29T05:30:05.290674Z",
     "iopub.status.idle": "2025-12-29T05:30:05.307454Z",
     "shell.execute_reply": "2025-12-29T05:30:05.306686Z",
     "shell.execute_reply.started": "2025-12-29T05:30:05.290933Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# TRAINING PIPELINE\n",
    "# ============================================================================\n",
    "def prepare_data(config: Config):\n",
    "    \"\"\"Load and prepare data with stratified group K-Fold splitting\"\"\"\n",
    "    train_df  = pd.read_csv(config.TRAIN_CSV)\n",
    "\n",
    "    # Create groups based on location/date to prevent data leakage\n",
    "    train_df['group'] = train_df['image_path'].str.extract(r'(ID\\d+)')[0]\n",
    "\n",
    "    # Stratify by total biomass bins\n",
    "    train_df_pivot = train_df.pivot_table(\n",
    "        index=['image_path', 'group'],\n",
    "        columns='target_name',\n",
    "        values='target'\n",
    "    ).reset_index()\n",
    "\n",
    "    # Create biomass bins for stratification\n",
    "    total_biomass = train_df_pivot['Dry_Total_g']\n",
    "    train_df_pivot['biomass_bin'] = pd.qcut(total_biomass, q=5, labels=False, duplicates='drop')\n",
    "    \n",
    "    # Group K-Fold\n",
    "    gkf = GroupKFold(n_splits=config.N_FOLDS)\n",
    "    train_df_pivot['fold'] = -1\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(gkf.split(\n",
    "        train_df_pivot, \n",
    "        train_df_pivot['biomass_bin'], \n",
    "        groups=train_df_pivot['group']\n",
    "    )):\n",
    "        train_df_pivot.loc[val_idx, 'fold'] = fold\n",
    "    \n",
    "    # Merge fold info back to original dataframe\n",
    "    fold_map = dict(zip(train_df_pivot['image_path'], train_df_pivot['fold']))\n",
    "    train_df['fold'] = train_df['image_path'].map(fold_map)\n",
    "\n",
    "    return train_df\n",
    "\n",
    "\n",
    "def train_fold(train_df: pd.DataFrame, fold: int, config: Config):\n",
    "    \"\"\"Train a single fold\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training Fold {fold}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "\n",
    "    # Split data\n",
    "    train_fold_df = train_df[train_df['fold'] != fold].reset_index(drop=True)\n",
    "    val_fold_df = train_df[train_df['fold'] == fold].reset_index(drop=True)\n",
    "\n",
    "    print(f\"Train samples: {len(train_fold_df)}\")\n",
    "    print(f\"Val samples: {len(val_fold_df)}\")\n",
    "\n",
    "    # Create datasets\n",
    "    train_dataset = BiomassDataset(\n",
    "        train_fold_df, \n",
    "        config.TRAIN_IMG_DIR,\n",
    "        transform=get_transforms(config.IMG_SIZE, mode='train'),\n",
    "        mode='train'\n",
    "    )\n",
    "\n",
    "    val_dataset = BiomassDataset(\n",
    "        val_fold_df,\n",
    "        config.TRAIN_IMG_DIR,\n",
    "        transform=get_transforms(config.IMG_SIZE, mode='val'),\n",
    "        mode='train'\n",
    "    )\n",
    "\n",
    "    # Create dataloaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=config.BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        num_workers=config.NUM_WORKERS,\n",
    "        pin_memory=True,\n",
    "        drop_last=True\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=config.BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        num_workers=config.NUM_WORKERS,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    # Create model\n",
    "    model = DinoBiomassModel(config)\n",
    "\n",
    "    # Callbacks\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        dirpath=f\"{config.OUTPUT_DIR}/fold_{fold}\",\n",
    "        filename='best-{epoch:02d}-{val_r2:.4f}',\n",
    "        monitor='val_r2',\n",
    "        mode='max',\n",
    "        save_top_k=1\n",
    "    )\n",
    "\n",
    "    early_stop_callback = EarlyStopping(\n",
    "        monitor='val_r2',\n",
    "        patience=10,\n",
    "        mode='max',\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    lr_monitor = LearningRateMonitor(logging_interval='epoch')\n",
    "\n",
    "    # Trainer\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=config.MAX_EPOCHS,\n",
    "        accelerator='auto',\n",
    "        devices='auto',\n",
    "        precision='16-mixed',\n",
    "        callbacks=[checkpoint_callback, early_stop_callback, lr_monitor],\n",
    "        log_every_n_steps=10,\n",
    "        deterministic=False\n",
    "    )\n",
    "\n",
    "    # Train\n",
    "    trainer.fit(model, train_loader, val_loader)\n",
    "\n",
    "    return checkpoint_callback.best_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T05:30:05.308630Z",
     "iopub.status.busy": "2025-12-29T05:30:05.308376Z",
     "iopub.status.idle": "2025-12-29T05:30:05.321887Z",
     "shell.execute_reply": "2025-12-29T05:30:05.321312Z",
     "shell.execute_reply.started": "2025-12-29T05:30:05.308593Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# INFERENCE\n",
    "# ============================================================================\n",
    "def predict(model_path: str, config: Config):\n",
    "    \"\"\"Generate predictions for test set\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"Generating Predictions\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "\n",
    "    # Load model\n",
    "    model = DinoBiomassModel.load_from_checkpoint(model_path, weights_only=False)\n",
    "    model.eval()\n",
    "    model.cuda()\n",
    "\n",
    "    # Load test data\n",
    "    test_df = pd.read_csv(config.TEST_CSV)\n",
    "    test_dataset = BiomassDataset(\n",
    "        test_df,\n",
    "        config.TEST_IMG_DIR,\n",
    "        transform=get_transforms(config.IMG_SIZE, mode='val'),\n",
    "        mode='test'\n",
    "    )\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=config.BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        num_workers=config.NUM_WORKERS,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    # Predict\n",
    "    predictions = []\n",
    "    image_paths = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, paths in test_loader:\n",
    "            images = images.cuda()\n",
    "            preds, _ = model(images)\n",
    "            predictions.append(preds.cpu().numpy())\n",
    "            image_paths.extend(paths)\n",
    "    \n",
    "    predictions = np.concatenate(predictions, axis=0)\n",
    "\n",
    "    # Create submission DataFrame\n",
    "    submission_rows = []\n",
    "    for idx, img_path in enumerate(image_paths):\n",
    "        img_id = Path(img_path).stem\n",
    "        for i, target_name in enumerate(config.TARGET_NAMES):\n",
    "            sample_id = f\"{img_id}__{target_name}\"\n",
    "            target_value = predictions[idx, i]\n",
    "            submission_rows.append({\n",
    "                'sample_id': sample_id,\n",
    "                'target': target_value\n",
    "            })\n",
    "    \n",
    "    submission_df = pd.DataFrame(submission_rows)\n",
    "    submission_df.to_csv(f\"{config.OUTPUT_DIR}/submission.csv\", index=False)\n",
    "    print(f\"Submission saved to {config.OUTPUT_DIR}/submission.csv\")\n",
    "\n",
    "    return submission_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T05:30:05.323842Z",
     "iopub.status.busy": "2025-12-29T05:30:05.323506Z",
     "iopub.status.idle": "2025-12-29T05:30:05.427373Z",
     "shell.execute_reply": "2025-12-29T05:30:05.426232Z",
     "shell.execute_reply.started": "2025-12-29T05:30:05.323819Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Training Fold 0\n",
      "============================================================\n",
      "\n",
      "Train samples: 1425\n",
      "Val samples: 360\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Config' object has no attribute 'BACKBONE'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_55/2730027891.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_55/2730027891.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFOLD_TO_TRAIN\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m# Train single fold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mbest_model_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_fold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFOLD_TO_TRAIN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0;31m# Generate predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_model_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_55/4254954423.py\u001b[0m in \u001b[0;36mtrain_fold\u001b[0;34m(train_df, fold, config)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;31m# Create model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDinoBiomassModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;31m# Callbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_55/708993477.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config, **kwargs)\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0;31m# Training mode: save config as individual hyperparameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             self.save_hyperparameters({\n\u001b[0;32m---> 84\u001b[0;31m                 \u001b[0;34m'backbone'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBACKBONE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m                 \u001b[0;34m'num_components'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNUM_COMPONENTS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0;34m'hidden_dim'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHIDDEN_DIM\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Config' object has no attribute 'BACKBONE'"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# MAIN\n",
    "# ============================================================================\n",
    "\n",
    "def main():\n",
    "    config = Config()\n",
    "    \n",
    "    # Create output directory\n",
    "    os.makedirs(config.OUTPUT_DIR, exist_ok=True)\n",
    "    \n",
    "    # Prepare data\n",
    "    train_df = prepare_data(config)\n",
    "    \n",
    "    # Train\n",
    "    if config.FOLD_TO_TRAIN is not None:\n",
    "        # Train single fold\n",
    "        best_model_path = train_fold(train_df, config.FOLD_TO_TRAIN, config)\n",
    "        # Generate predictions\n",
    "        predict(best_model_path, config)\n",
    "    else:\n",
    "        # Train all folds\n",
    "        best_models = []\n",
    "        for fold in range(config.N_FOLDS):\n",
    "            best_model_path = train_fold(train_df, fold, config)\n",
    "            best_models.append(best_model_path)\n",
    "        \n",
    "        print(\"\\nAll folds trained. Best models:\")\n",
    "        for i, path in enumerate(best_models):\n",
    "            print(f\"Fold {i}: {path}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 14254895,
     "isSourceIdPinned": false,
     "sourceId": 112509,
     "sourceType": "competition"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 986,
     "modelInstanceId": 3327,
     "sourceId": 4535,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31234,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
