{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db486fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T00:30:02.435139Z",
     "iopub.status.busy": "2026-01-01T00:30:02.434381Z",
     "iopub.status.idle": "2026-01-01T00:30:02.441477Z",
     "shell.execute_reply": "2026-01-01T00:30:02.440687Z",
     "shell.execute_reply.started": "2026-01-01T00:30:02.435108Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional, Literal\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from transformers import AutoModel, AutoConfig\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torchvision import transforms\n",
    "import h5py\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80dbc743",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T00:30:02.443371Z",
     "iopub.status.busy": "2026-01-01T00:30:02.443060Z",
     "iopub.status.idle": "2026-01-01T00:30:02.454641Z",
     "shell.execute_reply": "2026-01-01T00:30:02.453923Z",
     "shell.execute_reply.started": "2026-01-01T00:30:02.443352Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    'dino_path': '/kaggle/input/dinov2/pytorch/large/1',\n",
    "    'train_csv': '/kaggle/input/csiro-biomass/train.csv',\n",
    "    'train_img_dir': '/kaggle/input/csiro-biomass/',\n",
    "    'test_csv': '/kaggle/input/csiro-biomass/test.csv',\n",
    "    'test_img_dir': '/kaggle/input/csiro-biomass/test/',\n",
    "    'features_cache': 'biomass_features.h5',\n",
    "    'backbone_dim': 1024,\n",
    "    'img_size': 518,\n",
    "    'patch_stride': 400,\n",
    "    'num_registers': 4,\n",
    "    'batch_size': 4,\n",
    "    'learning_rate': 1e-4,\n",
    "    'weight_decay': 1e-5,\n",
    "    'num_epochs': 50,\n",
    "    'patience': 10,\n",
    "    'grad_clip': 1.0,\n",
    "    'extract_batch_size': 16,\n",
    "    'num_workers': 4,\n",
    "    'target_names': ['Dry_Green_g', 'Dry_Dead_g', 'Dry_Clover_g', 'GDM_g', 'Dry_Total_g'],\n",
    "    'target_weights': {\n",
    "        'Dry_Green_g': 0.1,\n",
    "        'Dry_Dead_g': 0.1,\n",
    "        'Dry_Clover_g': 0.1,\n",
    "        'GDM_g': 0.2,\n",
    "        'Dry_Total_g': 0.5\n",
    "    },\n",
    "    'states': ['NSW', 'Tas', 'Vic', 'WA'],\n",
    "    'use_cached_features': False, \n",
    "    'apply_frofa': True,  \n",
    "    'device': 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "}\n",
    "DEVICE = torch.device(CONFIG['device'])\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38cf9998",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T00:30:02.455766Z",
     "iopub.status.busy": "2026-01-01T00:30:02.455586Z",
     "iopub.status.idle": "2026-01-01T00:30:02.469810Z",
     "shell.execute_reply": "2026-01-01T00:30:02.469122Z",
     "shell.execute_reply.started": "2026-01-01T00:30:02.455750Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class FroFA:\n",
    "    def __init__(self, features: torch.Tensor, training: bool = True):\n",
    "        self.features = features\n",
    "        self.training = training\n",
    "    def apply_feature_augmentation(self):\n",
    "        if not self.training:\n",
    "            return self.features\n",
    "        if torch.rand(1).item() < 0.5:\n",
    "            noise = torch.randn_like(self.features) * 0.01\n",
    "            self.features += noise\n",
    "        if torch.rand(1).item() < 0.5:\n",
    "            scale = torch.rand(1).item() * 0.2 + 0.9\n",
    "            self.features *= scale\n",
    "        if torch.rand(1).item() < 0.3:\n",
    "            mask = torch.rand_like(self.features) > 0.1\n",
    "            self.features *= mask.float()\n",
    "        return self.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2cf7eea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T00:30:02.470916Z",
     "iopub.status.busy": "2026-01-01T00:30:02.470696Z",
     "iopub.status.idle": "2026-01-01T00:30:02.484035Z",
     "shell.execute_reply": "2026-01-01T00:30:02.483334Z",
     "shell.execute_reply.started": "2026-01-01T00:30:02.470897Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ZeroInflatedLogNormalLoss(nn.Module):\n",
    "    def __init__(self, eps: float = 1e-7):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "    def forward(self, preds: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
    "        logit_prob = preds[:, 0]\n",
    "        mu = torch.clamp(preds[:, 1], min=-10, max=10)\n",
    "        sigma = F.softplus(preds[:, 2]) + self.eps \n",
    "        sigma = torch.clamp(sigma, min=self.eps, max=10.0)   \n",
    "        is_positive = (target > 0).float()\n",
    "        class_loss = F.binary_cross_entropy_with_logits(logit_prob, is_positive, reduction='none')\n",
    "        safe_target = torch.clamp(target, min=self.eps)\n",
    "        log_target = torch.clamp(torch.log(safe_target), min=-10, max=10)\n",
    "        reg_loss = (0.5 * math.log(2 * math.pi) + torch.log(sigma) + (log_target - mu).pow(2) / (2 * sigma.pow(2)))\n",
    "        total_loss = class_loss + (is_positive * reg_loss)\n",
    "        return total_loss.mean()\n",
    "class TweedieLoss(nn.Module):\n",
    "    def __init__(self, p: float = 1.5, eps: float = 1e-8):\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "        self.eps = eps\n",
    "    def forward(self, pred: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
    "        pred = torch.clamp(pred, min=self.eps)\n",
    "        target = torch.clamp(target, min=self.eps)\n",
    "        term1 = -target * torch.pow(pred, 1 - self.p) / (1 - self.p)\n",
    "        term2 = torch.pow(pred, 2 - self.p) / (2 - self.p)\n",
    "        loss = term1 + term2\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a0559a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T00:30:02.622779Z",
     "iopub.status.busy": "2026-01-01T00:30:02.622587Z",
     "iopub.status.idle": "2026-01-01T00:30:02.630213Z",
     "shell.execute_reply": "2026-01-01T00:30:02.629500Z",
     "shell.execute_reply.started": "2026-01-01T00:30:02.622762Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class GatedAttentionMIL(nn.Module):\n",
    "    def __init__(self, input_dim: int = 1024, hidden_dim: int = 256):\n",
    "        super().__init__()\n",
    "        self.attention_V = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.attention_U = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.attention_weights = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        A_V = self.attention_V(x) \n",
    "        A_U = self.attention_U(x) \n",
    "        A = self.attention_weights(A_V * A_U)\n",
    "        A = torch.softmax(A, dim=1)\n",
    "        M = torch.bmm(A.transpose(1, 2), x)\n",
    "        return M.squeeze(1), A\n",
    "class FiLM(nn.Module):\n",
    "    def __init__(self, meta_dim: int, feat_dim: int):\n",
    "        super().__init__()\n",
    "        self.scale = nn.Linear(meta_dim, feat_dim)\n",
    "        self.shift = nn.Linear(meta_dim, feat_dim)\n",
    "    def forward(self, features: torch.Tensor, metadata: torch.Tensor) -> torch.Tensor:\n",
    "        gamma = self.scale(metadata) \n",
    "        beta = self.shift(metadata)  \n",
    "        return features * gamma + beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1816c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T00:30:02.631905Z",
     "iopub.status.busy": "2026-01-01T00:30:02.631604Z",
     "iopub.status.idle": "2026-01-01T00:30:02.644292Z",
     "shell.execute_reply": "2026-01-01T00:30:02.643752Z",
     "shell.execute_reply.started": "2026-01-01T00:30:02.631879Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class DINOv2Extractor(nn.Module):\n",
    "    def __init__(self, model_path: str = CONFIG['dino_path']):\n",
    "        super().__init__()\n",
    "        print(f\"Loading local DINOv2 from: {model_path}\")\n",
    "        self.config = AutoConfig.from_pretrained(model_path)\n",
    "        self.backbone = AutoModel.from_pretrained(model_path, config=self.config)\n",
    "        self.backbone.eval()\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.num_registers = 4\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        with torch.no_grad():\n",
    "            outputs = self.backbone(pixel_values=x)\n",
    "            if hasattr(outputs, 'last_hidden_state'):\n",
    "                last_hidden = outputs.last_hidden_state\n",
    "                cls_token = last_hidden[:, 0, :]\n",
    "            elif isinstance(outputs, dict):\n",
    "                cls_token = outputs.get('x_norm_clstoken', outputs['last_hidden_state'][:, 0, :])\n",
    "            else:\n",
    "                cls_token = outputs[:, 0, :]\n",
    "        return cls_token\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd7c046",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T00:30:02.645282Z",
     "iopub.status.busy": "2026-01-01T00:30:02.645059Z",
     "iopub.status.idle": "2026-01-01T00:30:02.662342Z",
     "shell.execute_reply": "2026-01-01T00:30:02.661705Z",
     "shell.execute_reply.started": "2026-01-01T00:30:02.645257Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class AFHN(nn.Module):\n",
    "    def __init__(self, num_components: int = 5, meta_dim: int = 8, feat_dim: int = 1024, use_ziln: bool = True, online_mode: bool = True):\n",
    "        super().__init__()\n",
    "        self.num_components = num_components\n",
    "        self.use_ziln = use_ziln\n",
    "        self.online_mode = online_mode\n",
    "        if online_mode:\n",
    "            config = AutoConfig.from_pretrained(CONFIG['dino_path'])\n",
    "            self.backbone = AutoModel.from_pretrained(CONFIG['dino_path'], config=config)\n",
    "            for param in self.backbone.parameters():\n",
    "                param.requires_grad = False\n",
    "        else:\n",
    "            self.backbone = None\n",
    "        self.vis_dim = feat_dim\n",
    "        self.mil = GatedAttentionMIL(input_dim=self.vis_dim, hidden_dim=256)\n",
    "        self.meta_encoder = nn.Sequential(\n",
    "            nn.Linear(meta_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 64)\n",
    "        )\n",
    "        self.film = FiLM(meta_dim=64, feat_dim=self.vis_dim)\n",
    "        if self.use_ziln:\n",
    "            self.head_total = nn.Sequential(\n",
    "                nn.Linear(self.vis_dim, 256),\n",
    "                nn.GELU(),\n",
    "                nn.Dropout(0.2),\n",
    "                nn.Linear(256, 3)\n",
    "            )\n",
    "        else:\n",
    "            self.head_total = nn.Sequential(\n",
    "                nn.Linear(self.vis_dim, 256),\n",
    "                nn.GELU(),\n",
    "                nn.Dropout(0.2),\n",
    "                nn.Linear(256, 1),\n",
    "                nn.Softplus() \n",
    "            )\n",
    "        self.head_ratios = nn.Sequential(\n",
    "            nn.Linear(self.vis_dim, 256),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, num_components)\n",
    "        )\n",
    "        self.head_gate = nn.Sequential(\n",
    "            nn.Linear(self.vis_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, num_components)\n",
    "        )\n",
    "    def extract_features(self, x_patches: torch.Tensor) -> torch.Tensor:\n",
    "        if not self.online_mode:\n",
    "            raise RuntimeError(\"extract_features() only available in online mode\")\n",
    "        batch_size, n_patches, c, h, w = x_patches.shape\n",
    "        x_flat = x_patches.view(batch_size * n_patches, c, h, w)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.backbone(pixel_values=x_flat)\n",
    "            if hasattr(outputs, 'last_hidden_state'):\n",
    "                feat_flat = outputs.last_hidden_state[:, 0, :]\n",
    "            else:\n",
    "                feat_flat = outputs[:, 0, :]\n",
    "        feat_seq = feat_flat.view(batch_size, n_patches, -1)\n",
    "        return feat_seq\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        metadata: torch.Tensor,\n",
    "        apply_frofa: bool = False\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        if self.online_mode:\n",
    "            if x.dim() == 5: \n",
    "                features = self.extract_features(x)\n",
    "            else:  \n",
    "                features = x\n",
    "        else:\n",
    "            features = x \n",
    "        if apply_frofa and CONFIG['apply_frofa']:\n",
    "            frofa = FroFA(features, apply_frofa)\n",
    "            features = frofa.apply_feature_augmentation()\n",
    "        global_feat, _ = self.mil(features)\n",
    "        meta_emb = self.meta_encoder(metadata)\n",
    "        modulated_feat = self.film(global_feat, meta_emb)\n",
    "        total = self.head_total(modulated_feat)\n",
    "        raw_ratios = self.head_ratios(modulated_feat)\n",
    "        ratios = F.softmax(raw_ratios, dim=1)\n",
    "        gate_logits = self.head_gate(modulated_feat)\n",
    "        return {\n",
    "            'total': total,\n",
    "            'ratios': ratios,\n",
    "            'gates': gate_logits\n",
    "        }\n",
    "    def predict_components(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        metadata: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        outputs = self.forward(x, metadata, apply_frofa=False)\n",
    "        if self.use_ziln:\n",
    "            total_ziln = outputs['total']\n",
    "            prob_nonzero = torch.sigmoid(total_ziln[:, 0])\n",
    "            mu = total_ziln[:, 1]\n",
    "            sigma = F.softplus(total_ziln[:, 2]) + 1e-6\n",
    "            expected_total = prob_nonzero * torch.exp(mu + 0.5 * sigma.pow(2))\n",
    "        else:\n",
    "            expected_total = outputs['total'].squeeze(-1)\n",
    "        gates = torch.sigmoid(outputs['gates'])\n",
    "        gated_ratios = outputs['ratios'] * gates\n",
    "        sum_gated = gated_ratios.sum(dim=1, keepdim=True) + 1e-6\n",
    "        final_ratios = gated_ratios / sum_gated\n",
    "        components = expected_total.unsqueeze(1) * final_ratios\n",
    "        return components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d684c044",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T00:30:02.663295Z",
     "iopub.status.busy": "2026-01-01T00:30:02.663041Z",
     "iopub.status.idle": "2026-01-01T00:30:02.684758Z",
     "shell.execute_reply": "2026-01-01T00:30:02.684140Z",
     "shell.execute_reply.started": "2026-01-01T00:30:02.663277Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class PatchBiomassDataset(Dataset):    \n",
    "    def __init__(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        image_dir: str,\n",
    "        transform: Optional[A.Compose] = None,\n",
    "        patch_size: int = 518,\n",
    "        stride: int = 400,\n",
    "        is_test: bool = False\n",
    "    ):\n",
    "        if is_test:\n",
    "            self.df = df.groupby('image_path').first().reset_index()\n",
    "        else:\n",
    "            self.df = df.groupby('sample_id').first().reset_index()\n",
    "        self.image_dir = Path(image_dir)\n",
    "        self.transform = transform\n",
    "        self.patch_size = patch_size\n",
    "        self.stride = stride\n",
    "        self.is_test = is_test\n",
    "        self.state_to_idx = {s: i for i, s in enumerate(CONFIG['states'])}\n",
    "    def extract_patches(self, image: np.ndarray) -> torch.Tensor:\n",
    "        h, w = image.shape[:2]\n",
    "        patches = []\n",
    "        for y in range(0, max(1, h - self.patch_size + 1), self.stride):\n",
    "            for x in range(0, max(1, w - self.patch_size + 1), self.stride):\n",
    "                y_end = min(y + self.patch_size, h)\n",
    "                x_end = min(x + self.patch_size, w)\n",
    "                y_start = max(0, y_end - self.patch_size)\n",
    "                x_start = max(0, x_end - self.patch_size)\n",
    "                patch = image[y_start:y_end, x_start:x_end]\n",
    "                if patch.shape[0] < self.patch_size or patch.shape[1] < self.patch_size:\n",
    "                    pad_h = self.patch_size - patch.shape[0]\n",
    "                    pad_w = self.patch_size - patch.shape[1]\n",
    "                    patch = np.pad(patch, ((0, pad_h), (0, pad_w), (0, 0)), mode='reflect')\n",
    "                if self.transform:\n",
    "                    augmented = self.transform(image=patch)\n",
    "                    patch_tensor = augmented['image']\n",
    "                else:\n",
    "                    patch_tensor = torch.from_numpy(patch).permute(2, 0, 1).float() / 255.0\n",
    "                patches.append(patch_tensor)\n",
    "        return torch.stack(patches)\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.df)\n",
    "    def __getitem__(self, idx: int) -> Dict:\n",
    "        row = self.df.iloc[idx]\n",
    "        img_path = self.image_dir / row['image_path']\n",
    "        image = np.array(Image.open(img_path).convert('RGB'))\n",
    "        patches = self.extract_patches(image)\n",
    "        state_onehot = np.zeros(len(CONFIG['states']))\n",
    "        if row['State'] in self.state_to_idx:\n",
    "            state_onehot[self.state_to_idx[row['State']]] = 1.0\n",
    "        date = pd.to_datetime(row['Sampling_Date'])\n",
    "        month = date.month\n",
    "        month_sin = np.sin(2 * np.pi * month / 12)\n",
    "        month_cos = np.cos(2 * np.pi * month / 12)\n",
    "        metadata = torch.tensor(\n",
    "            [row['Height_Ave_cm'], row['Pre_GSHH_NDVI']] + \n",
    "            state_onehot.tolist() + \n",
    "            [month_sin, month_cos],\n",
    "            dtype=torch.float32\n",
    "        )\n",
    "        if not self.is_test:\n",
    "            targets = torch.tensor([\n",
    "                row['Dry_Green_g'],\n",
    "                row['Dry_Dead_g'],\n",
    "                row['Dry_Clover_g'],\n",
    "                row['GDM_g'],\n",
    "                row['Dry_Total_g']\n",
    "            ], dtype=torch.float32)\n",
    "            return {\n",
    "                'patches': patches,\n",
    "                'metadata': metadata,\n",
    "                'targets': targets,\n",
    "                'sample_id': row['sample_id']\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                'patches': patches,\n",
    "                'metadata': metadata,\n",
    "                'sample_id': row.name\n",
    "            }\n",
    "        \n",
    "class CachedFeaturesDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        hdf5_path: str,\n",
    "        df: pd.DataFrame,\n",
    "        is_test: bool = False\n",
    "    ):\n",
    "        self.hdf5_path = hdf5_path\n",
    "        self.is_test = is_test\n",
    "        if 'sample_id' in df.columns:\n",
    "            df['sample_id'] = df['sample_id'].str.split('__').str[0]\n",
    "        if is_test:\n",
    "            self.df = df.groupby('image_path').first().reset_index()\n",
    "        else:\n",
    "            self.df = df.groupby('sample_id').first().reset_index()\n",
    "        self.h5_file = h5py.File(hdf5_path, 'r', swmr=True)\n",
    "        self.state_to_idx = {s: i for i, s in enumerate(CONFIG['states'])}\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.df)\n",
    "    def __getitem__(self, idx: int) -> Dict:\n",
    "        row = self.df.iloc[idx]\n",
    "        sample_id = row.get('sample_id', str(idx))\n",
    "        try:\n",
    "            grp = self.h5_file[str(sample_id)]\n",
    "            features = torch.from_numpy(grp['features'][:]).float()\n",
    "            height = float(grp.attrs.get('Height_Ave_cm', row.get('Height_Ave_cm', 0)))\n",
    "            ndvi = float(grp.attrs.get('Pre_GSHH_NDVI', row.get('Pre_GSHH_NDVI', 0)))\n",
    "            state = str(grp.attrs.get('State', row.get('State', 'Unknown')))\n",
    "            date_str = str(grp.attrs.get('Sampling_Date', row.get('Sampling_Date', '2020-01-01')))\n",
    "        except KeyError:\n",
    "            features = torch.zeros(1, CONFIG['backbone_dim'])\n",
    "            height = float(row.get('Height_Ave_cm', 0))\n",
    "            ndvi = float(row.get('Pre_GSHH_NDVI', 0))\n",
    "            state = str(row.get('State', 'Unknown'))\n",
    "            date_str = str(row.get('Sampling_Date', '2020-01-01'))\n",
    "        state_onehot = np.zeros(len(CONFIG['states']))\n",
    "        if state in self.state_to_idx:\n",
    "            state_onehot[self.state_to_idx[state]] = 1.0\n",
    "        try:\n",
    "            date = pd.to_datetime(date_str)\n",
    "            month = date.month\n",
    "        except:\n",
    "            month = 1\n",
    "        month_sin = np.sin(2 * np.pi * month / 12)\n",
    "        month_cos = np.cos(2 * np.pi * month / 12)\n",
    "        metadata = torch.tensor(\n",
    "            [height, ndvi] + state_onehot.tolist() + [month_sin, month_cos],\n",
    "            dtype=torch.float32\n",
    "        )\n",
    "        if not self.is_test:\n",
    "            targets = torch.tensor([\n",
    "                row['Dry_Green_g'],\n",
    "                row['Dry_Dead_g'],\n",
    "                row['Dry_Clover_g'],\n",
    "                row['GDM_g'],\n",
    "                row['Dry_Total_g']\n",
    "            ], dtype=torch.float32)\n",
    "            \n",
    "            return {\n",
    "                'features': features,\n",
    "                'metadata': metadata,\n",
    "                'targets': targets,\n",
    "                'sample_id': sample_id\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                'features': features,\n",
    "                'metadata': metadata,\n",
    "                'sample_id': sample_id\n",
    "            }\n",
    "    def __del__(self):\n",
    "        if hasattr(self, 'h5_file'):\n",
    "            self.h5_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3bc2a25",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T00:30:02.686594Z",
     "iopub.status.busy": "2026-01-01T00:30:02.686155Z",
     "iopub.status.idle": "2026-01-01T00:30:02.697893Z",
     "shell.execute_reply": "2026-01-01T00:30:02.697240Z",
     "shell.execute_reply.started": "2026-01-01T00:30:02.686575Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_transforms(is_train: bool = True) -> A.Compose:\n",
    "    if is_train:\n",
    "        return A.Compose([\n",
    "            A.RandomResizedCrop(\n",
    "                size=(CONFIG['img_size'], CONFIG['img_size']),\n",
    "                scale=(0.7, 1.0),\n",
    "                ratio=(0.9, 1.1),\n",
    "                p=1.0\n",
    "            ),\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.VerticalFlip(p=0.5),\n",
    "            A.RandomRotate90(p=0.5),\n",
    "            A.Transpose(p=0.5),\n",
    "            A.OneOf([\n",
    "                A.MotionBlur(p=0.2),\n",
    "                A.GaussianBlur(p=0.2),\n",
    "            ], p=0.2),\n",
    "            A.OneOf([\n",
    "                A.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.05, p=0.3),\n",
    "                A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n",
    "                A.RandomGamma(gamma_limit=(80, 120), p=0.3),\n",
    "                A.HueSaturationValue(p=0.3),\n",
    "            ], p=0.3),\n",
    "            A.GaussNoise(var_limit=(10.0, 50.0), p=0.2),\n",
    "            A.CoarseDropout(max_holes=8, max_height=32, max_width=32, fill_value=0, p=0.3),\n",
    "            A.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225]\n",
    "            ),\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "    else:\n",
    "        return A.Compose([\n",
    "            A.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225]\n",
    "            ),\n",
    "            ToTensorV2()\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932d7318",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T00:30:02.699235Z",
     "iopub.status.busy": "2026-01-01T00:30:02.698958Z",
     "iopub.status.idle": "2026-01-01T00:30:02.713480Z",
     "shell.execute_reply": "2026-01-01T00:30:02.712778Z",
     "shell.execute_reply.started": "2026-01-01T00:30:02.699182Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_epoch(\n",
    "    model: AFHN,\n",
    "    dataloader: DataLoader,\n",
    "    optimizer: optim.Optimizer,\n",
    "    criterion_ziln: ZeroInflatedLogNormalLoss,\n",
    "    device: torch.device,\n",
    "    use_cached: bool = False\n",
    ") -> float:\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    pbar = tqdm(dataloader, desc='Training')\n",
    "    for batch in pbar:\n",
    "        if use_cached:\n",
    "            x = batch['features'].to(device)\n",
    "        else:\n",
    "            x = batch['patches'].to(device)\n",
    "        metadata = batch['metadata'].to(device)\n",
    "        targets = batch['targets'].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(x, metadata, apply_frofa=True)\n",
    "        pred_components = model.predict_components(x, metadata)\n",
    "        loss_components = F.mse_loss(pred_components, targets)\n",
    "        target_total = targets[:, -1]\n",
    "        loss_total = criterion_ziln(outputs['total'], target_total)\n",
    "        target_gates = (targets > 0).float()\n",
    "        loss_gate = F.binary_cross_entropy_with_logits(\n",
    "            outputs['gates'], target_gates\n",
    "        )\n",
    "        gt_total = target_total.unsqueeze(1) + 1e-6\n",
    "        gt_ratios = targets / gt_total\n",
    "        pred_gates = torch.sigmoid(outputs['gates'])\n",
    "        gated_ratios = outputs['ratios'] * pred_gates\n",
    "        sum_gated = gated_ratios.sum(dim=1, keepdim=True) + 1e-6\n",
    "        pred_effective_ratios = gated_ratios / sum_gated\n",
    "        loss_ratios = F.mse_loss(pred_effective_ratios, gt_ratios)\n",
    "        loss = loss_total + loss_ratios + 0.5 * loss_gate + 0.1 * loss_components\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), CONFIG['grad_clip'])\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def validate(\n",
    "    model: AFHN,\n",
    "    dataloader: DataLoader,\n",
    "    device: torch.device,\n",
    "    use_cached: bool = False\n",
    ") -> Tuple[float, Dict[str, float]]:\n",
    "    model.eval()\n",
    "    all_targets = []\n",
    "    all_predictions = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc='Validating'):\n",
    "            if use_cached:\n",
    "                x = batch['features'].to(device)\n",
    "            else:\n",
    "                x = batch['patches'].to(device)\n",
    "            metadata = batch['metadata'].to(device)\n",
    "            targets = batch['targets']\n",
    "            predictions = model.predict_components(x, metadata)\n",
    "            all_targets.append(targets)\n",
    "            all_predictions.append(predictions.cpu())\n",
    "    all_targets = torch.cat(all_targets)\n",
    "    all_predictions = torch.cat(all_predictions)\n",
    "    targets_np = all_targets.numpy()\n",
    "    preds_np = all_predictions.numpy()\n",
    "    n_samples = len(targets_np)\n",
    "    weights = np.array([CONFIG['target_weights'][name] for name in CONFIG['target_names']])\n",
    "    weights_tiled = np.tile(weights, n_samples)\n",
    "    targets_flat = targets_np.flatten()\n",
    "    preds_flat = preds_np.flatten()\n",
    "    y_mean = np.average(targets_flat, weights=weights_tiled)\n",
    "    ss_res = np.sum(weights_tiled * (targets_flat - preds_flat) ** 2)\n",
    "    ss_tot = np.sum(weights_tiled * (targets_flat - y_mean) ** 2)\n",
    "    r2 = 1 - (ss_res / (ss_tot + 1e-8))\n",
    "    metrics = {'Weighted_R2': r2}\n",
    "    for i, name in enumerate(CONFIG['target_names']):\n",
    "        mae = np.abs(targets_np[:, i] - preds_np[:, i]).mean()\n",
    "        metrics[f'{name}_MAE'] = mae\n",
    "    return r2, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5183004",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T00:30:02.811815Z",
     "iopub.status.busy": "2026-01-01T00:30:02.811149Z",
     "iopub.status.idle": "2026-01-01T00:30:02.826917Z",
     "shell.execute_reply": "2026-01-01T00:30:02.826242Z",
     "shell.execute_reply.started": "2026-01-01T00:30:02.811790Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ImageTilingDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        image_dir: str,\n",
    "        patch_size: int = 518,\n",
    "        stride: int = 400\n",
    "    ):\n",
    "        self.df = df\n",
    "        self.image_dir = Path(image_dir)\n",
    "        self.patch_size = patch_size\n",
    "        self.stride = stride\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225]\n",
    "            )\n",
    "        ])\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.df)\n",
    "    def extract_patches(self, image: np.ndarray) -> torch.Tensor:\n",
    "        h, w = image.shape[:2]\n",
    "        patches = []\n",
    "        for y in range(0, max(1, h - self.patch_size + 1), self.stride):\n",
    "            for x in range(0, max(1, w - self.patch_size + 1), self.stride):\n",
    "                y_end = min(y + self.patch_size, h)\n",
    "                x_end = min(x + self.patch_size, w)\n",
    "                y_start = max(0, y_end - self.patch_size)\n",
    "                x_start = max(0, x_end - self.patch_size)\n",
    "                patch = image[y_start:y_end, x_start:x_end]\n",
    "                if patch.shape[0] < self.patch_size or patch.shape[1] < self.patch_size:\n",
    "                    pad_h = self.patch_size - patch.shape[0]\n",
    "                    pad_w = self.patch_size - patch.shape[1]\n",
    "                    patch = np.pad(patch, ((0, pad_h), (0, pad_w), (0, 0)), mode='reflect')\n",
    "                patch_pil = Image.fromarray(patch)\n",
    "                patch_tensor = self.transform(patch_pil)\n",
    "                patches.append(patch_tensor)\n",
    "        return torch.stack(patches)\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, str, Dict]:\n",
    "        row = self.df.iloc[idx]\n",
    "        img_path = self.image_dir / row['image_path']\n",
    "        try:\n",
    "            image = np.array(Image.open(img_path).convert('RGB'))\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {img_path}: {e}\")\n",
    "            return torch.zeros(1, 3, self.patch_size, self.patch_size), str(idx), {}\n",
    "        patches = self.extract_patches(image)\n",
    "        metadata = {\n",
    "            'Height_Ave_cm': float(row.get('Height_Ave_cm', 0)),\n",
    "            'Pre_GSHH_NDVI': float(row.get('Pre_GSHH_NDVI', 0)),\n",
    "            'State': str(row.get('State', 'Unknown')),\n",
    "            'Sampling_Date': str(row.get('Sampling_Date', '2020-01-01'))\n",
    "        }\n",
    "        sample_id = row.get('sample_id', str(idx))\n",
    "        return patches, sample_id, metadata\n",
    "def extract_and_cache_features(\n",
    "    csv_path: str,\n",
    "    image_dir: str,\n",
    "    output_hdf5: str,\n",
    "    device: torch.device = DEVICE\n",
    "):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    if 'sample_id' in df.columns:\n",
    "        df['sample_id'] = df['sample_id'].str.split('__').str[0]\n",
    "    df_unique = df.groupby('sample_id').first().reset_index()\n",
    "    print(f\"Found {len(df_unique)} unique images\")\n",
    "    dataset = ImageTilingDataset(df_unique, image_dir, patch_size=CONFIG['img_size'], stride=CONFIG['patch_stride'])\n",
    "    dataloader = DataLoader(dataset, batch_size=1, shuffle=False, num_workers=CONFIG['num_workers'], pin_memory=True)\n",
    "    model = DINOv2Extractor(CONFIG['dino_path']).to(device)\n",
    "    print(f\"Creating HDF5 cache: {output_hdf5}\")\n",
    "    with h5py.File(output_hdf5, 'w') as f:\n",
    "        for patches, sample_id, metadata in tqdm(dataloader, desc=\"Extracting features\"):\n",
    "            sample_id = sample_id[0]\n",
    "            patches = patches.squeeze(0).to(device)\n",
    "            if patches.dim() != 4:\n",
    "                print(f\"Skipping {sample_id}: invalid shape\")\n",
    "                continue\n",
    "            features_list = []\n",
    "            batch_size = CONFIG['extract_batch_size']\n",
    "            for i in range(0, len(patches), batch_size):\n",
    "                batch = patches[i:i+batch_size]\n",
    "                feat = model(batch)\n",
    "                features_list.append(feat.cpu())\n",
    "            features = torch.cat(features_list, dim=0).numpy()\n",
    "            grp = f.create_group(sample_id)\n",
    "            grp.create_dataset('features', data=features, compression='gzip', compression_opts=4)\n",
    "            for key, value in metadata.items():\n",
    "                if isinstance(value, list):\n",
    "                    value = value[0]\n",
    "                grp.attrs[key] = value\n",
    "    print(f\"\\nFeature extraction complete!\")\n",
    "    print(f\"Saved to: {output_hdf5}\")\n",
    "    print(f\"File size: {os.path.getsize(output_hdf5) / 1024**2:.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76473751",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T00:30:02.828396Z",
     "iopub.status.busy": "2026-01-01T00:30:02.828159Z",
     "iopub.status.idle": "2026-01-01T00:30:02.843690Z",
     "shell.execute_reply": "2026-01-01T00:30:02.843082Z",
     "shell.execute_reply.started": "2026-01-01T00:30:02.828379Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    train_df = pd.read_csv(CONFIG['train_csv'])\n",
    "    train_df['sample_id'] = train_df['sample_id'].str.split('__').str[0]\n",
    "    train_df_wide = train_df.pivot_table(\n",
    "        index=['sample_id', 'image_path', 'Sampling_Date', 'State',\n",
    "               'Species', 'Pre_GSHH_NDVI', 'Height_Ave_cm'],\n",
    "        columns='target_name',\n",
    "        values='target'\n",
    "    ).reset_index()\n",
    "    train_df_wide = train_df_wide.fillna(0.0)\n",
    "    groups = train_df_wide['State'].values\n",
    "    n_groups = len(np.unique(groups))\n",
    "    n_splits = min(5, n_groups)\n",
    "    gkf = GroupKFold(n_splits=n_splits)\n",
    "    fold = 0\n",
    "    for train_idx, val_idx in gkf.split(train_df_wide, groups=groups):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Fold {fold + 1}/{n_splits}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        train_fold = train_df_wide.iloc[train_idx]\n",
    "        val_fold = train_df_wide.iloc[val_idx]\n",
    "        if CONFIG['use_cached_features']:\n",
    "            print(\"Using cached features from HDF5\")\n",
    "            train_dataset = CachedFeaturesDataset(\n",
    "                CONFIG['features_cache'], train_fold, is_test=False\n",
    "            )\n",
    "            val_dataset = CachedFeaturesDataset(\n",
    "                CONFIG['features_cache'], val_fold, is_test=False\n",
    "            )\n",
    "            online_mode = False\n",
    "        else:\n",
    "            print(\"Using online training (loading images)\")\n",
    "            train_dataset = PatchBiomassDataset(\n",
    "                train_fold, CONFIG['train_img_dir'],\n",
    "                transform=get_transforms(is_train=True)\n",
    "            )\n",
    "            val_dataset = PatchBiomassDataset(\n",
    "                val_fold, CONFIG['train_img_dir'],\n",
    "                transform=get_transforms(is_train=False)\n",
    "            )\n",
    "            online_mode = True\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset, batch_size=CONFIG['batch_size'],\n",
    "            shuffle=True, num_workers=CONFIG['num_workers'], pin_memory=True\n",
    "        )\n",
    "        val_loader = DataLoader(\n",
    "            val_dataset, batch_size=CONFIG['batch_size'],\n",
    "            shuffle=False, num_workers=CONFIG['num_workers'], pin_memory=True\n",
    "        )\n",
    "        model = AFHN(\n",
    "            num_components=5,\n",
    "            meta_dim=8,\n",
    "            feat_dim=CONFIG['backbone_dim'],\n",
    "            online_mode=online_mode\n",
    "        ).to(DEVICE)\n",
    "        optimizer = optim.AdamW(\n",
    "            model.parameters(),\n",
    "            lr=CONFIG['learning_rate'],\n",
    "            weight_decay=CONFIG['weight_decay']\n",
    "        )\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "            optimizer, T_0=10, T_mult=2\n",
    "        )\n",
    "        criterion_ziln = ZeroInflatedLogNormalLoss()\n",
    "        best_r2 = -float('inf')\n",
    "        patience_counter = 0\n",
    "        for epoch in range(CONFIG['num_epochs']):\n",
    "            train_loss = train_epoch(\n",
    "                model, train_loader, optimizer, criterion_ziln, \n",
    "                DEVICE, use_cached=CONFIG['use_cached_features']\n",
    "            )\n",
    "            val_r2, val_metrics = validate(\n",
    "                model, val_loader, DEVICE,\n",
    "                use_cached=CONFIG['use_cached_features']\n",
    "            )\n",
    "            scheduler.step()\n",
    "            print(f\"Train Loss: {train_loss:.4f}\")\n",
    "            print(f\"Val Weighted R²: {val_r2:.4f}\")\n",
    "            for k, v in val_metrics.items():\n",
    "                if k != 'Weighted_R2':\n",
    "                    print(f\"  {k}: {v:.4f}\")\n",
    "            if val_r2 > best_r2:\n",
    "                best_r2 = val_r2\n",
    "                patience_counter = 0\n",
    "                model_name = f'afhn_{\"cached\" if CONFIG[\"use_cached_features\"] else \"online\"}_fold{fold}_best.pth'\n",
    "                torch.save(model.state_dict(), model_name)\n",
    "                print(f\"Saved best model (R²={best_r2:.4f})\")\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= CONFIG['patience']:\n",
    "                    print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "                    break\n",
    "        print(f\"\\nFold {fold + 1} Best R²: {best_r2:.4f}\")\n",
    "        fold += 1\n",
    "        # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d759ea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T00:30:02.844659Z",
     "iopub.status.busy": "2026-01-01T00:30:02.844418Z",
     "iopub.status.idle": "2026-01-01T00:32:41.481976Z",
     "shell.execute_reply": "2026-01-01T00:32:41.481239Z",
     "shell.execute_reply.started": "2026-01-01T00:30:02.844641Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # MODE 1: Extract features (run once)\n",
    "    # extract_and_cache_features(\n",
    "    #     csv_path=CONFIG['train_csv'],\n",
    "    #     image_dir=CONFIG['train_img_dir'],\n",
    "    #     output_hdf5=CONFIG['features_cache']\n",
    "    # )\n",
    "    \n",
    "    # MODE 2: Train on cached features (fast)\n",
    "    CONFIG['use_cached_features'] = True\n",
    "    main()\n",
    "    \n",
    "    # MODE 3: Train online (traditional, slower but self-contained)\n",
    "    # CONFIG['use_cached_features'] = False\n",
    "    # main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75fee6e2-5d5f-42a0-b1f6-a1faf1b47a39",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def inference():\n",
    "    \"\"\"Inference on test set.\"\"\"\n",
    "    # Load test data\n",
    "    test_df = pd.read_csv(CONFIG['test_csv'])\n",
    "\n",
    "    test_dataset = PatchBiomassDataset(\n",
    "        test_df, CONFIG['test_img_dir'],\n",
    "        transform=get_transforms(is_train=False),\n",
    "        is_test=True\n",
    "    )\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, batch_size=4,\n",
    "        shuffle=False, num_workers=4\n",
    "    )\n",
    "\n",
    "    # Load model\n",
    "    model = AFHN(num_components=5, meta_dim=8, use_ziln=True).to(DEVICE)\n",
    "    model.load_state_dict(torch.load(MODEL_PATH))\n",
    "\n",
    "    # Predict\n",
    "    predictions = []\n",
    "    sample_ids = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc='Inference'):\n",
    "            patches = batch['patches'].to(DEVICE)\n",
    "            metadata = batch['metadata'].to(DEVICE)\n",
    "            \n",
    "            preds = model.predict_components(patches, metadata)\n",
    "            predictions.append(preds.cpu().numpy())\n",
    "            sample_ids.extend(batch['sample_id'])\n",
    "    \n",
    "    predictions = np.vstack(predictions)\n",
    "\n",
    "    # Create submission\n",
    "    submission_rows = []\n",
    "\n",
    "    for i, img_id in enumerate(sample_ids):\n",
    "        for j, target_name in enumerate(TARGET_NAMES):\n",
    "            submission_rows.append({\n",
    "                'sample_id': f\"{img_id}__{target_name}\",\n",
    "                'target': predictions[i, j]\n",
    "            })\n",
    "    \n",
    "    submission_df = pd.DataFrame(submission_rows)\n",
    "    submission_df.to_csv('submission.csv', index=False)\n",
    "    print(\"Submission saved to submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ee61d1-a35b-4dce-b1e5-6e4ac0cc203e",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "inference()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 14254895,
     "sourceId": 112509,
     "sourceType": "competition"
    },
    {
     "modelId": 986,
     "modelInstanceId": 3327,
     "sourceId": 4535,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31236,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
